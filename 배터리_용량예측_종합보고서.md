# ğŸ”‹ ì¤‘ê³  ë°°í„°ë¦¬ ì…€ ìš©ëŸ‰(Capacity) ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ ì¢…í•© ë³´ê³ ì„œ

> **í”„ë¡œì íŠ¸**: ì¤‘ê³  ë°°í„°ë¦¬ ì…€ì˜ ì´ˆê¸° ì¸¡ì • ë°ì´í„° ê¸°ë°˜ ì‹¤ì œ ë°©ì „ ìš©ëŸ‰ ì˜ˆì¸¡  
> **ì‘ì„±ì¼**: 2026ë…„ 2ì›” 21ì¼  
> **ë°ì´í„°**: BS-LSBAT-S240629 ë°°ì¹˜, 1,040ê°œ ë°°í„°ë¦¬ ì…€  
> **ë¶„ì„ í™˜ê²½**: Python 3.14.2 Â· XGBoost 3.2.0 Â· LightGBM 4.6.0 Â· Optuna 4.7.0 Â· scikit-learn 1.8.0

---

## ëª©ì°¨

1. [Phase 1: ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ (EDA)](#phase-1-ë°ì´í„°-íƒìƒ‰-ë°-ë¶„ì„-eda)
2. [Phase 2: íŒŒìƒë³€ìˆ˜ ìƒì„± (Feature Engineering)](#phase-2-íŒŒìƒë³€ìˆ˜-ìƒì„±-feature-engineering)
3. [Phase 3: ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì„ íƒ](#phase-3-ë°ì´í„°-ì „ì²˜ë¦¬-ë°-í”¼ì²˜-ì„ íƒ)
4. [Phase 4: ìµœì¢… í•™ìŠµ ë°ì´í„° êµ¬ì„±](#phase-4-ìµœì¢…-í•™ìŠµ-ë°ì´í„°-êµ¬ì„±)
5. [Phase 5: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€](#phase-5-ëª¨ë¸-í•™ìŠµ-ë°-í‰ê°€)
6. [Phase 7: Knowledge Distillation](#phase-7-knowledge-distillation)
7. [Phase 8: ì„±ëŠ¥ ê³ ë„í™” ì‹¤í—˜](#phase-8-ì„±ëŠ¥-ê³ ë„í™”-ì‹¤í—˜)
8. [Phase 9: S1 ì§‘ì¤‘ ê°œì„ ](#phase-9-s1-ì§‘ì¤‘-ê°œì„ )
9. [Phase 10: KD ì •ë³´ì±„ë„ ë³´ì™„](#phase-10-kd-ì •ë³´ì±„ë„-ë³´ì™„)
10. [ì¢…í•© ê²°ë¡  ë° ì‹¤ë¬´ ì œì–¸](#ì¢…í•©-ê²°ë¡ -ë°-ì‹¤ë¬´-ì œì–¸)

---

# Phase 1: ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ (EDA)

ì¤‘ê³  ë°°í„°ë¦¬ ì…€ì˜ ì´ˆê¸° ì¸¡ì • ë°ì´í„°(ì „ì••, AC ì„í”¼ë˜ìŠ¤)ë¡œë¶€í„° ì‹¤ì œ ë°©ì „ ìš©ëŸ‰(Capacity)ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ì…ë‹ˆë‹¤.

---

## 1-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° í™˜ê²½ ì„¤ì •

```python
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from scipy import stats
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False
COLORS = sns.color_palette('husl', 10)
```

**ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬:**

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ìš©ë„ |
|---|---|
| `pandas` | ë°ì´í„°í”„ë ˆì„ ì¡°ì‘ |
| `numpy` | ìˆ˜ì¹˜ ì—°ì‚° |
| `matplotlib` / `seaborn` | ì‹œê°í™” |
| `scipy.stats` | í†µê³„ ê²€ì • (Z-score ë“±) |

> âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ìƒ ë¡œë“œ, í•œê¸€ í°íŠ¸(`Malgun Gothic`) ì„¤ì • ì™„ë£Œ

---

## 1-2. ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ì •ë¦¬

```python
raw = pd.read_csv('./data.csv')
# ì»¬ëŸ¼ ì´ë¦„ ì¬ì •ì˜ â†’ ë¼ë²¨/ë¹ˆ ì»¬ëŸ¼ ì œê±° â†’ ìˆ˜ì¹˜í˜• ë³€í™˜
```

**ì¶œë ¥:**
```
ë°ì´í„° Shape: (1040, 10)
í”¼ì²˜ ëª©ë¡: ['initial_voltage', 'initial_impedance', 'v42_voltage', 'v42_impedance',
            'v25_voltage', 'v25_impedance', 'v36_voltage', 'v36_impedance']
íƒ€ê²Ÿ: capacity
```

**ë°ì´í„° ìƒìœ„ 5í–‰ (df.head()):**

| cell_id | initial_voltage | initial_impedance | v42_voltage | v42_impedance | v25_voltage | v25_impedance | v36_voltage | v36_impedance | capacity |
|---------|-----------------|-------------------|-------------|---------------|-------------|---------------|-------------|---------------|----------|
| BS-LSBAT-S240629-0001 | 3.4552 | 11.39 | 4.1774 | 10.56 | 2.9610 | 10.55 | 3.5900 | 11.08 | 4.976 |
| BS-LSBAT-S240629-0002 | 3.4546 | 11.61 | 4.1779 | 10.70 | 2.9772 | 10.75 | 3.5900 | 10.62 | 5.012 |
| BS-LSBAT-S240629-0003 | 3.4554 | 10.97 | 4.1779 | 10.42 | 2.9656 | 10.65 | 3.5900 | 10.87 | 5.000 |
| BS-LSBAT-S240629-0004 | 3.4570 | 10.93 | 4.1782 | 10.31 | 2.9663 | 10.71 | 3.5900 | 10.73 | 5.004 |
| BS-LSBAT-S240629-0005 | 3.4555 | 11.24 | 4.1774 | 10.52 | 2.9714 | 11.12 | 3.5900 | 10.97 | 5.009 |

**í•´ì„:**
- **ë°ì´í„° í¬ê¸°**: 1,040ê°œ ì…€ Ã— 10ê°œ ì»¬ëŸ¼ (cell_id + 8ê°œ í”¼ì²˜ + 1ê°œ íƒ€ê²Ÿ)
- **í”¼ì²˜ êµ¬ì„±**: 4ê°œ ì¸¡ì • êµ¬ê°„(Initial, 4.2V, 2.5V, 3.6V) Ã— 2ê°œ ì¸¡ì •ê°’(ì „ì••, ì„í”¼ë˜ìŠ¤) = **8ê°œ ì›ë³¸ í”¼ì²˜**
- **ì…€ ID íŒ¨í„´**: `BS-LSBAT-S240629-XXXX` í˜•ì‹ìœ¼ë¡œ ë™ì¼ ë°°ì¹˜ì˜ ë°°í„°ë¦¬ ì…€

| êµ¬ê°„ | ì˜ë¯¸ | ì „ì•• ëŒ€í‘œê°’ | ì„í”¼ë˜ìŠ¤ ëŒ€í‘œê°’ |
|---|---|---|---|
| Initial (OCV) | ì´ˆê¸° ê°œë°©íšŒë¡œì „ì•• | 3.455~3.457V | 10.93~11.61mÎ© |
| 4.2V (Full) | ë§Œì¶© ìƒíƒœ | 4.177~4.178V | 10.31~10.56mÎ© |
| 2.5V (Cut-off) | ë°©ì „ ì¢…ì§€ | 2.961~2.977V | 10.55~11.12mÎ© |
| 3.6V (Nominal) | ê³µì¹­ ì „ì•• ë¶€ê·¼ | 3.59V | 10.62~11.08mÎ© |

---

## 1-3. ê¸°ë³¸ í†µê³„ ë¶„ì„

```python
stats_df = df[numeric_cols].describe().T
stats_df['missing'] = df[numeric_cols].isnull().sum()
stats_df['skewness'] = df[numeric_cols].skew()
stats_df['kurtosis'] = df[numeric_cols].kurtosis()
```

**ì¶œë ¥ (ê¸°ë³¸ í†µê³„ ìš”ì•½ í…Œì´ë¸”):**

| í”¼ì²˜ | mean | std | min | max | missing | skew | kurtosis |
|------|------|-----|-----|-----|---------|------|----------|
| initial_voltage | 3.4560 | 0.0018 | 3.4512 | 3.4622 | 0 | 0.44 | 0.19 |
| initial_impedance | 11.40 | 0.82 | 9.47 | 14.14 | 0 | 0.23 | -0.28 |
| v42_voltage | 4.1780 | 0.0030 | 4.1705 | 4.1940 | 0 | 2.65 | 8.41 |
| v42_impedance | 10.67 | 0.40 | 9.79 | 12.16 | 0 | 0.69 | 0.25 |
| v25_voltage | 2.9700 | 0.0216 | 2.8922 | 3.0282 | 0 | -0.17 | -0.44 |
| v25_impedance | 10.81 | 0.32 | 9.88 | 11.86 | 0 | 0.05 | -0.10 |
| v36_voltage | 3.5899 | 0.0007 | 3.5852 | 3.5930 | 0 | -2.42 | 27.07 |
| v36_impedance | 11.89 | 31.59 | 10.14 | 1032.0 | 0 | 32.24 | 1040.0 |
| **capacity** | **5.026** | **0.033** | **4.792** | **5.097** | **0** | **-2.16** | **7.84** |

**í•µì‹¬ í•´ì„:**
1. **ê²°ì¸¡ì¹˜**: ì „ì²´ 0ê°œ â†’ ë³„ë„ì˜ ê²°ì¸¡ê°’ ì²˜ë¦¬ ë¶ˆí•„ìš”
2. **v42_voltage** / **v36_voltage**: í‘œì¤€í¸ì°¨ê°€ ë§¤ìš° ì‘ìŒ â†’ ì‚¬ì‹¤ìƒ ìƒìˆ˜ì— ê°€ê¹Œì›€
3. **v25_voltage**: ìƒëŒ€ì ìœ¼ë¡œ ë³€ë™ì„±ì´ ë†’ì•„ **ì…€ ê°„ ì°¨ì´ë¥¼ ê°€ì¥ ì˜ ë°˜ì˜**
4. **v36_impedance**: min 10.14 / max 1032.0 â†’ **ê·¹ë‹¨ ì´ìƒì¹˜ 1ê°œ ì¡´ì¬** (std=31.59)
5. **Capacity (íƒ€ê²Ÿ)**: í‰ê·  5.026 Ah, í‘œì¤€í¸ì°¨ 0.033 Ah, ë²”ìœ„ 4.792~5.097 â†’ **ì•½ 6% ì´ë‚´ì˜ ë§¤ìš° ì¢ì€ ë³€ë™**
6. Capacityì˜ ìŒì˜ ì™œë„(skew=-2.16): **ì™¼ìª½ ê¼¬ë¦¬ ë¶„í¬** â€” ëŒ€ë¶€ë¶„ ê³µì¹­ ìš©ëŸ‰ì— ê°€ê¹ê³ , ì¼ë¶€ ì—´í™” ì…€ì€ ë‚®ì€ ê°’

---

## 1-4. ë¶„í¬ ì‹œê°í™” (íˆìŠ¤í† ê·¸ë¨ + KDE)

```python
fig, axes = plt.subplots(3, 3, figsize=(18, 14))
for i, col in enumerate(numeric_cols):
    ax.hist(data, bins=50, alpha=0.6, density=True)
    data.plot.kde(ax=ax, color='black', linewidth=1.5)
    ax.axvline(data.mean(), color='red', linestyle='--')   # í‰ê· 
    ax.axvline(data.median(), color='blue', linestyle='--')  # ì¤‘ì•™ê°’
plt.savefig('./eda_01_distributions.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_01_distributions.png`

![í”¼ì²˜ë³„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨+KDE](eda_01_distributions.png)

**ê° í”¼ì²˜ ë¶„í¬ íŠ¹ì„±:**

| í”¼ì²˜ | ë¶„í¬ í˜•íƒœ | íŠ¹ì´ì‚¬í•­ |
|---|---|---|
| `initial_voltage` | ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ | í‰ê·  3.456V ë¶€ê·¼ ì§‘ì¤‘ |
| `initial_impedance` | ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ ë¶„í¬ | ì¼ë¶€ ë†’ì€ ì„í”¼ë˜ìŠ¤ ê°’ ì¡´ì¬ |
| `v42_voltage` | ë§¤ìš° ì¢ì€ ë²”ìœ„ ì§‘ì¤‘ | 4.178V ë¶€ê·¼ì— ë°€ì§‘, ì‚¬ì‹¤ìƒ ìƒìˆ˜ |
| `v42_impedance` | ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ | ëŒ€ë¶€ë¶„ 10~11mÎ©, ì¼ë¶€ ë†’ì€ ê°’ |
| `v25_voltage` | **ê°€ì¥ ë„“ì€ ë¶„í¬** | **ì…€ ì—´í™” ì°¨ì´ë¥¼ ê°€ì¥ ì˜ ë°˜ì˜** |
| `v25_impedance` | ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ | 10~11mÎ© ë²”ìœ„ |
| `v36_voltage` | ì¢ì€ ë²”ìœ„ | 3.59V ë¶€ê·¼ ë°€ì§‘ |
| `v36_impedance` | ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ | 10~11mÎ© (ê·¹ë‹¨ ì´ìƒì¹˜ 1ê°œ ì œì™¸) |
| `capacity` | ì™¼ìª½ ê¼¬ë¦¬ | ëŒ€ë¶€ë¶„ 5.0Ah ì´ìƒ, ì¼ë¶€ ì—´í™” ì…€ 4.8Ah ì´í•˜ |

> **í•µì‹¬**: `v42_voltage`ì™€ `v36_voltage`ëŠ” ë³€ë™ì„±ì´ ê±°ì˜ ì—†ì–´ ì˜ˆì¸¡ ê¸°ì—¬ë„ê°€ ë‚®ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ. ë°˜ë©´ `v25_voltage`ëŠ” ê°€ì¥ ë„“ì€ ë¶„í¬ë¡œ ì˜ˆì¸¡ ë³€ìˆ˜ë¡œì„œ ê°€ì¥ ìœ ë§.

---

## 1-5. ë°•ìŠ¤í”Œë¡¯ (ì´ìƒì¹˜ í™•ì¸)

```python
fig, axes = plt.subplots(3, 3, figsize=(18, 12))
for i, col in enumerate(numeric_cols):
    bp = ax.boxplot(df[col].dropna(), patch_artist=True)
    # IQR ì´ìƒì¹˜ ìˆ˜ í‘œì‹œ
plt.savefig('./eda_02_boxplots.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_02_boxplots.png`

![í”¼ì²˜ë³„ ë°•ìŠ¤í”Œë¡¯](eda_02_boxplots.png)

**í•´ì„:**
- **`v42_voltage`**: ë°•ìŠ¤ê°€ ë§¤ìš° ì¢ê³  ìœ„ìŠ¤ì»¤ ë°–ì— ë‹¤ìˆ˜ì˜ ì´ìƒì¹˜ â†’ ëŒ€ë¶€ë¶„ 4.178Vì— ë°€ì§‘
- **`v42_impedance`**: ìœ„ìª½ ì´ìƒì¹˜ ë‹¤ìˆ˜ â†’ ì¼ë¶€ ì…€ì˜ ë‚´ë¶€ì €í•­ì´ ì¶©ì „ í›„ì—ë„ ë†’ê²Œ ìœ ì§€
- **`v36_voltage`**: ê°€ì¥ ë§ì€ ì´ìƒì¹˜ â†’ ë¶„í¬ê°€ ë§¤ìš° ì¢ì•„ IQRì´ ì‘ê¸° ë•Œë¬¸ì— ê³¼ë„ ê²€ì¶œ
- **`capacity`**: ì•„ë˜ìª½ ì´ìƒì¹˜ â†’ ê³µì¹­ ìš©ëŸ‰ë³´ë‹¤ í˜„ì €íˆ ë‚®ì€ ì—´í™” ì…€

> **ì£¼ì˜**: IQR ê¸°ë°˜ ì´ìƒì¹˜ëŠ” ë¶„í¬ê°€ ì¢ì€ í”¼ì²˜ì—ì„œ ê³¼ë„í•˜ê²Œ ê²€ì¶œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, **ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜** íŒë‹¨ì´ ë³‘í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

---

## 1-6. ìƒê´€ë¶„ì„ (Pearson / Spearman)

```python
fig, axes = plt.subplots(1, 2, figsize=(20, 8))
# Pearson ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ
sns.heatmap(pearson_corr, mask=mask, annot=True, fmt='.3f', cmap='RdBu_r', ...)
# Spearman ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ
sns.heatmap(spearman_corr, mask=mask, annot=True, fmt='.3f', cmap='RdBu_r', ...)
plt.savefig('./eda_03_correlation.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_03_correlation.png`

![Pearson & Spearman ìƒê´€ë¶„ì„ íˆíŠ¸ë§µ](eda_03_correlation.png)

**Pearson vs Spearman ë¹„êµ:**

| ê´€ì°° ê²°ê³¼ | ë‚´ìš© |
|---|---|
| ì„í”¼ë˜ìŠ¤ í”¼ì²˜ ê°„ | ë†’ì€ ì–‘ì˜ ìƒê´€ê´€ê³„ â†’ **ë‹¤ì¤‘ê³µì„ ì„±** ìš°ë ¤ (ê°™ì€ ë¬¼ë¦¬ëŸ‰ì„ ë‹¤ë¥¸ ì¡°ê±´ì—ì„œ ì¸¡ì •) |
| ì „ì•• í”¼ì²˜ ê°„ | ì¼ë¶€ ë†’ì€ ìƒê´€, ì¼ë¶€ ì•½í•œ ìƒê´€ |
| ì „ì•• â†” ì„í”¼ë˜ìŠ¤ | ëšœë ·í•œ ìƒê´€ â†’ ì „ê·¹ ì—´í™”: ì „ì••â†“, ì„í”¼ë˜ìŠ¤â†‘ |
| Pearson â‰ˆ Spearman | ëŒ€ì²´ë¡œ ìœ ì‚¬ â†’ ëŒ€ë¶€ë¶„ **ì„ í˜• ê´€ê³„**ê°€ ì§€ë°°ì  |

---

### Capacityì™€ì˜ ìƒê´€ê³„ìˆ˜ ìˆœìœ„

```python
corr_with_target = pd.DataFrame({
    'Pearson': pearson_corr[TARGET].drop(TARGET),
    'Spearman': spearman_corr[TARGET].drop(TARGET),
}).sort_values('|Pearson|', ascending=False)
```

**ì¶œë ¥ (Capacity ìƒê´€ê³„ìˆ˜ í…Œì´ë¸” - ì ˆëŒ€ê°’ ë‚´ë¦¼ì°¨ìˆœ):**

| ìˆœìœ„ | í”¼ì²˜ | Pearson r | |Pearson| | |Spearman| | í•´ì„ |
|---|---|---|---|---|---|
| 1 | `v25_voltage` | **-0.5952** | **0.5952** | 0.5842 | ğŸŸ¢ **ê°€ì¥ ê°•í•œ ìƒê´€** |
| 2 | `v25_impedance` | -0.2130 | 0.2130 | 0.2150 | ğŸŸ¡ ë³´í†µ |
| 3 | `v42_impedance` | -0.1773 | 0.1773 | 0.1760 | ğŸŸ¡ ë³´í†µ |
| 4 | `initial_impedance` | -0.1499 | 0.1499 | 0.1399 | ğŸ”´ ì•½í•¨ |
| 5 | `initial_voltage` | +0.1365 | 0.1365 | 0.1353 | ğŸ”´ ì•½í•¨ |
| 6 | `v36_impedance` | -0.0211 | 0.0211 | 0.0367 | ğŸ”´ ë¬´ì‹œ ê°€ëŠ¥ |
| 7 | `v42_voltage` | -0.0173 | 0.0173 | 0.0192 | ğŸ”´ ë¬´ì‹œ ê°€ëŠ¥ |
| 8 | `v36_voltage` | +0.0137 | 0.0137 | 0.0125 | ğŸ”´ ë¬´ì‹œ ê°€ëŠ¥ |

> **í•µì‹¬**: `v25_voltage`(ë°©ì „ ì¢…ì§€ ì „ì••)ê°€ Capacityì™€ **ìœ ì¼í•˜ê²Œ ì¤‘ê°„ ì´ìƒì˜ ìƒê´€ê´€ê³„**(|r| > 0.5)ë¥¼ ë³´ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ì›ë³¸ í”¼ì²˜ê°€ ì•½í•œ ìƒê´€ì´ë¯€ë¡œ **íŒŒìƒë³€ìˆ˜ ìƒì„±**ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

---

## 1-7. í”¼ì²˜ vs Capacity ì‚°ì ë„

```python
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
for i, col in enumerate(FEATURES):
    ax.scatter(df[col], df[TARGET], alpha=0.3, s=10)
    # ì¶”ì„¸ì„  + ìƒê´€ê³„ìˆ˜ í‘œì‹œ
plt.savefig('./eda_04_scatter.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_04_scatter.png`

![í”¼ì²˜ vs Capacity ì‚°ì ë„](eda_04_scatter.png)

**8ê°œ í”¼ì²˜ë³„ ì‚°ì ë„ íŒ¨í„´:**
- **`v25_voltage` (r=-0.595)**: ê°€ì¥ ëšœë ·í•œ **ìŒì˜ ì„ í˜• ê´€ê³„**. ë°©ì „ ì¢…ì§€ ì „ì••ì´ ë†’ì„ìˆ˜ë¡ Capacityê°€ ë‚®ì€ ê²½í–¥.
- **ì„í”¼ë˜ìŠ¤ í”¼ì²˜ë“¤ (r â‰ˆ -0.15 ~ -0.21)**: ì•½í•œ ìŒì˜ ê²½í–¥. ë°ì´í„°ê°€ ë„“ê²Œ í¼ì ¸ ìˆì–´ ëª…í™•í•œ íŒ¨í„´ í™•ì¸ ì–´ë ¤ì›€.
- **`v42_voltage`, `v36_voltage`**: ê±°ì˜ ìˆ˜ì§ ë¶„í¬ â†’ ì‚¬ì‹¤ìƒ ìƒìˆ˜, ì˜ˆì¸¡ ê¸°ì—¬ ë¯¸ë¯¸.

---

## 1-8. ì´ìƒì¹˜ íƒì§€ (IQR + Z-score)

```python
def detect_outliers(df, columns):
    """IQR ë° Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€"""
    for col in columns:
        Q1, Q3 = data.quantile(0.25), data.quantile(0.75)
        IQR = Q3 - Q1
        iqr_outliers = ((data < Q1 - 1.5 * IQR) | (data > Q3 + 1.5 * IQR)).sum()
        z_outliers = (np.abs(stats.zscore(data)) > 3).sum()
```

**ì¶œë ¥ (ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ í…Œì´ë¸”):**

| í”¼ì²˜ | IQR ì´ìƒì¹˜ ìˆ˜ | IQR ë¹„ìœ¨(%) | Z-score>3 | Z ë¹„ìœ¨(%) |
|------|-------------|------------|-----------|----------|
| initial_voltage | 12 | 1.15 | 0 | 0.00 |
| initial_impedance | 6 | 0.58 | 0 | 0.00 |
| v42_voltage | 57 | 5.48 | 0 | 0.00 |
| v42_impedance | 73 | 7.02 | 0 | 0.00 |
| v25_voltage | 17 | 1.63 | 0 | 0.00 |
| v25_impedance | 2 | 0.19 | 0 | 0.00 |
| v36_voltage | 138 | 13.27 | 10 | 0.96 |
| v36_impedance | 1 | 0.10 | 1 | 0.10 |
| capacity | 20 | 1.92 | 0 | 0.00 |

**ì´ìƒì¹˜ ì…€ ìƒì„¸ (IQR ê¸°ì¤€):**

```
[v42_voltage] ì´ìƒì¹˜ 57ê°œ
  - BS-LSBAT-S240629-0006: v42_voltage=4.1933
  - BS-LSBAT-S240629-0015: v42_voltage=4.1932
  ... ì™¸ 52ê°œ

[v42_impedance] ì´ìƒì¹˜ 73ê°œ
  - BS-LSBAT-S240629-0009: v42_impedance=11.20
  ... ì™¸ 68ê°œ

[v25_voltage] ì´ìƒì¹˜ 17ê°œ
  - BS-LSBAT-S240629-0217: v25_voltage=3.0282
  ... ì™¸ 12ê°œ
```

**ì „ì²´ ì´ìƒì¹˜ í˜„í™©:**
```
ğŸ“Œ í•˜ë‚˜ ì´ìƒì˜ í”¼ì²˜ì—ì„œ ì´ìƒì¹˜ì¸ ì…€ ìˆ˜: 379ê°œ / ì „ì²´ 1040ê°œ
   ì´ìƒì¹˜ ë¹„ìœ¨: 36.4%
```

> **í•´ì„**: ì´ìƒì¹˜ ë¹„ìœ¨ì´ 36.4%ë¡œ ë†’ì§€ë§Œ, ì´ëŠ” ë¶„í¬ê°€ ì¢ì€ í”¼ì²˜(`v42_voltage`, `v36_voltage`)ì—ì„œ ê³¼ë„ ê²€ì¶œëœ ê²°ê³¼ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ **ì…€ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ í¸ì°¨**ì´ë¯€ë¡œ, ë‹¨ìˆœ ì œê±° ëŒ€ì‹  **Robust Scaler** ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

---

## 1-9. ì´ìƒì¹˜ í•˜ì´ë¼ì´íŠ¸ ì‹œê°í™”

```python
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
for i, col in enumerate(FEATURES):
    ax.scatter(df.loc[~mask, col], df.loc[~mask, TARGET], color='steelblue', label='ì •ìƒ')
    ax.scatter(df.loc[mask, col], df.loc[mask, TARGET], color='red', marker='x', label='ì´ìƒì¹˜')
plt.savefig('./eda_05_outliers.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_05_outliers.png`

![ì´ìƒì¹˜ í•˜ì´ë¼ì´íŠ¸ ì‚°ì ë„](eda_05_outliers.png)

**í•´ì„:**
- ì´ìƒì¹˜(ë¹¨ê°„ x)ê°€ íŠ¹ì • Capacity êµ¬ê°„ì— ì§‘ì¤‘ë˜ì§€ ì•Šê³  ì „ë°˜ì ìœ¼ë¡œ ë¶„í¬
- ì´ìƒì¹˜ì™€ ì •ìƒê°’ì´ ë¶„í¬ì˜ ì—°ì¥ì„ ìƒì— ìœ„ì¹˜ â†’ **ìì—° í¸ì°¨**ì´ì§€ ì¸¡ì • ì˜¤ë¥˜ê°€ ì•„ë‹˜
- `v42_voltage`, `v36_voltage`ì—ì„œ ì´ìƒì¹˜ê°€ ë§ì´ ë³´ì´ì§€ë§Œ, ì‹¤ì œ ê°’ ì°¨ì´ëŠ” 0.01V ìˆ˜ì¤€

---

## 1-10. í”¼ì²˜ ê·¸ë£¹ë³„ íŒ¨í„´ ë¶„ì„

```python
fig, axes = plt.subplots(1, 2, figsize=(16, 6))
# ì¢Œ: ì¸¡ì • êµ¬ê°„ë³„ ì „ì•• ë°•ìŠ¤í”Œë¡¯
# ìš°: ì¸¡ì • êµ¬ê°„ë³„ ì„í”¼ë˜ìŠ¤ ë°•ìŠ¤í”Œë¡¯
plt.savefig('./eda_06_feature_groups.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_06_feature_groups.png`

![ì¸¡ì • êµ¬ê°„ë³„ ì „ì••/ì„í”¼ë˜ìŠ¤ ë¹„êµ](eda_06_feature_groups.png)

**ì¢Œì¸¡ - ì „ì•• ë¶„í¬:**
- Initial(OCV) â‰ˆ **3.456V**: ì…€ì˜ SOC(ì¶©ì „ìƒíƒœ) ë°˜ì˜
- 4.2V(Full) â‰ˆ **4.178V**: ë§Œì¶© ì „ì••ì€ ê±°ì˜ ë™ì¼ â†’ ì¶©ì „ í”„ë¡œí† ì½œì´ ì¼ì •
- 2.5V(Cut-off) â‰ˆ **2.97V**: ë°©ì „ ì¢…ì§€ ì „ì••ì˜ ë¶„í¬ê°€ **ê°€ì¥ ë„“ìŒ** â†’ **ì…€ ê±´ê°•ìƒíƒœ ì°¨ì´ ë°˜ì˜**
- 3.6V(Nominal) â‰ˆ **3.59V**: ë³€ë™ì„± ë§¤ìš° ë‚®ìŒ

**ìš°ì¸¡ - AC ì„í”¼ë˜ìŠ¤ ë¶„í¬:**
- 4ê°œ ì¸¡ì • êµ¬ê°„ ëª¨ë‘ **10~12mÎ©** ë²”ìœ„
- Initialì—ì„œ ì¤‘ì•™ê°’ì´ ë†’ê³ , 4.2Vì—ì„œ ì•½ê°„ ê°ì†Œ â†’ **ì¶©ì „ ì‹œ ë‚´ë¶€ì €í•­ ê°ì†Œ**í•˜ëŠ” ì „ê¸°í™”í•™ì  íŠ¹ì„±

---

## 1-11. ì„í”¼ë˜ìŠ¤ Pairplot

```python
pair_cols = impedance_cols + [TARGET]
g = sns.pairplot(df[pair_cols].dropna(), diag_kind='kde', plot_kws={'alpha': 0.3, 's': 10})
plt.savefig('./eda_07_pairplot.png')
```

**ğŸ“Š ì‹œê°í™”:** `eda_07_pairplot.png`

![ì„í”¼ë˜ìŠ¤ í”¼ì²˜ ê°„ ê´€ê³„ Pairplot](eda_07_pairplot.png)

**í•´ì„:**
- **ì„í”¼ë˜ìŠ¤ í”¼ì²˜ ê°„**: ê°•í•œ ì–‘ì˜ ì„ í˜• ê´€ê³„ â†’ í•œ ì¡°ê±´ì—ì„œ ë‚´ë¶€ì €í•­ì´ ë†’ì€ ì…€ì€ ë‹¤ë¥¸ ì¡°ê±´ì—ì„œë„ ë†’ìŒ
- **ì„í”¼ë˜ìŠ¤ â†’ Capacity**: ì•½í•œ ìŒì˜ ê²½í–¥ â†’ ë‚´ë¶€ì €í•­ ë†’ìœ¼ë©´ ìš©ëŸ‰ ë‚®ì§€ë§Œ, ì‚°ì ë„ê°€ ë„“ê²Œ í¼ì ¸ ì˜ˆì¸¡ë ¥ ì œí•œì 
- **ë‹¤ì¤‘ê³µì„ ì„± ê²½ê³ **: 4ê°œ ì„í”¼ë˜ìŠ¤ í”¼ì²˜ ê°„ ìƒê´€ì´ ë†’ì•„ ì „ë¶€ íˆ¬ì…í•˜ë©´ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆì•ˆì •

---

## 1-12. Phase 1 EDA ìš”ì•½

```
======================================================================
ğŸ“‹ Phase 1 EDA ìš”ì•½ ë¦¬í¬íŠ¸
======================================================================
ğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´
   Total Samples: 1040
   Features: 8ê°œ
   ê²°ì¸¡ì¹˜: 0ê°œ

ğŸ¯ Capacity (íƒ€ê²Ÿ) ë¶„í¬
   í‰ê· : 5.0262 Ah
   í‘œì¤€í¸ì°¨: 0.0329 Ah
   ë²”ìœ„: 4.7917 ~ 5.0968 Ah

ğŸ“ˆ Capacityì™€ ìƒê´€ê³„ìˆ˜ TOP-5 (Pearson)
   v25_voltage:        r=-0.5952
   v25_impedance:      r=-0.2130
   v42_impedance:      r=-0.1773
   initial_impedance:  r=-0.1499
   initial_voltage:    r=+0.1365

ğŸš¨ ì´ìƒì¹˜ í˜„í™©
   ì´ìƒì¹˜ í¬í•¨ ì…€: 379ê°œ (36.4%)
   ê°€ì¥ ì´ìƒì¹˜ê°€ ë§ì€ í”¼ì²˜: v36_voltage (138ê°œ)

ğŸ’¡ ì‹œì‚¬ì 
   1. ì„í”¼ë˜ìŠ¤ í”¼ì²˜ë“¤ì´ Capacityì™€ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ë³´ìœ 
   2. ì „ì•• í”¼ì²˜ë“¤ì€ ëŒ€ë¶€ë¶„ ë™ì¼í•œ ê°’ìœ¼ë¡œ ìˆ˜ë ´ (ë³€ë™ì„± ë‚®ìŒ)
   3. ì´ìƒì¹˜ ì…€ ì¡´ì¬ â†’ ì „ì²˜ë¦¬ ì‹œ ì²˜ë¦¬ ë°©ì¹¨ ê²°ì • í•„ìš”
   4. í”¼ì²˜ ê°„ ë‹¤ì¤‘ê³µì„ ì„±ì´ ë†’ì„ ìˆ˜ ìˆìŒ â†’ VIF ë¶„ì„ í•„ìš”
======================================================================
```

| í•­ëª© | ê²°ê³¼ | ì˜ë¯¸ |
|---|---|---|
| ë°ì´í„° í’ˆì§ˆ | ê²°ì¸¡ì¹˜ 0, 1040ê°œ ì…€ | ì „ì²˜ë¦¬ ë¶€ë‹´ ë‚®ìŒ |
| íƒ€ê²Ÿ ë¶„í¬ | 5.026Â±0.033 Ah | ë§¤ìš° ì¢ì€ ë²”ìœ„ â†’ ì •ë°€í•œ ëª¨ë¸ í•„ìš” |
| ìµœê°• ì˜ˆì¸¡ì | `v25_voltage` (r=-0.595) | ë°©ì „ ì¢…ì§€ ì „ì••ì´ ìœ ì¼í•œ ì¤‘ê°„ ìƒê´€ í”¼ì²˜ |
| ì´ìƒì¹˜ | 36.4% (379/1040) | IQR ê¸°ì¤€ ê³¼ë„ ê²€ì¶œ, ëŒ€ë¶€ë¶„ ìì—° í¸ì°¨ |
| ë‹¤ì¤‘ê³µì„ ì„± | ì„í”¼ë˜ìŠ¤ í”¼ì²˜ ê°„ ë†’ì€ ìƒê´€ | VIF ë¶„ì„ ë° í”¼ì²˜ ì„ íƒ í•„ìš” |

---

# Phase 2: íŒŒìƒë³€ìˆ˜ ìƒì„± (Feature Engineering)

ì›ë³¸ 8ê°œ í”¼ì²˜ì˜ ë‹¨ë… ì˜ˆì¸¡ë ¥ì´ ëŒ€ë¶€ë¶„ ì•½í•˜ë¯€ë¡œ, ë°°í„°ë¦¬ ë„ë©”ì¸ ì§€ì‹ì— ê¸°ë°˜í•œ **16ê°œ íŒŒìƒë³€ìˆ˜**ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

**ê·¼ê±°:**
- **ì„í”¼ë˜ìŠ¤ íŒŒìƒë³€ìˆ˜**: EIS ê¸°ë°˜ ì„í”¼ë˜ìŠ¤ íŒŒë¼ë¯¸í„° ë³€í™”ê°€ SOHì™€ ë‹¨ì¡° ì¦ê°€ ê´€ê³„ (2024, MDPI Batteries)
- **ì „ì•• íŒŒìƒë³€ìˆ˜**: OCV ë³€í™”ê°€ ë°°í„°ë¦¬ ì—´í™”ì™€ ì§ì ‘ì  ìƒê´€, OCV-SOC ê³¡ì„ ì˜ ë³€í™”ê°€ SOH ì§€í‘œ
- **ë³µí•© íŒŒìƒë³€ìˆ˜**: ì „ì••/ì„í”¼ë˜ìŠ¤ ê²°í•© ì§€í‘œë¡œ ì¢…í•©ì  ì…€ ê±´ê°•ìƒíƒœ ì¶”ì •

---

## 2-1. ì„í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ (8ê°œ)

```python
df['impedance_delta_42'] = df[z_42] - df[z_initial]
df['impedance_delta_25'] = df[z_25] - df[z_initial]
df['impedance_delta_36'] = df[z_36] - df[z_initial]
df['impedance_ratio_42'] = df[z_42] / df[z_initial]
df['impedance_ratio_25'] = df[z_25] / df[z_initial]
df['impedance_range'] = df[impedance_cols].max(axis=1) - df[impedance_cols].min(axis=1)
df['impedance_mean'] = df[impedance_cols].mean(axis=1)
df['impedance_std'] = df[impedance_cols].std(axis=1)
```

**ì¶œë ¥ (ì„í”¼ë˜ìŠ¤ íŒŒìƒë³€ìˆ˜ í†µê³„):**

| íŒŒìƒë³€ìˆ˜ | ìˆ˜ì‹ | í‰ê·  | í‘œì¤€í¸ì°¨ | Capacity r | í•´ì„ |
|---|---|---|---|---|---|
| `impedance_delta_42` | Zâ‚„.â‚‚V âˆ’ Z_initial | -0.680 | 0.294 | -0.005 | ì¶©ì „ ì‹œ ë‚´ë¶€ì €í•­ ê°ì†Œ(-0.68mÎ©): ì •ìƒ |
| `impedance_delta_25` | Zâ‚‚.â‚…V âˆ’ Z_initial | -0.391 | 0.255 | -0.034 | ë°©ì „ ì‹œì—ë„ ê°ì†Œ |
| `impedance_delta_36` | Zâ‚ƒ.â‚†V âˆ’ Z_initial | +0.389 | **31.82** | +0.019 | âš ï¸ ê·¹ë‹¨ ì´ìƒì¹˜(max=1020.82) |
| `impedance_ratio_42` | Zâ‚„.â‚‚V / Z_initial | 0.941 | 0.025 | -0.010 | ì¶©ì „ ì‹œ ~94%ë¡œ ê°ì†Œ |
| `impedance_ratio_25` | Zâ‚‚.â‚…V / Z_initial | 0.966 | 0.022 | -0.037 | ë°©ì „ ì‹œ ~97% ìœ ì§€ |
| `impedance_range` | Z_max âˆ’ Z_min | 1.839 | **31.79** | +0.020 | âš ï¸ ì´ìƒì¹˜ ì™œê³¡ |
| `impedance_mean` | mean(Z_all) | 11.217 | **7.96** | +0.013 | í‰ê·  ì„í”¼ë˜ìŠ¤ |
| `impedance_std` | std(Z_all) | 0.882 | **15.89** | +0.020 | âš ï¸ ì´ìƒì¹˜ ì™œê³¡ |

> **í•µì‹¬**: ì„í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ëŠ” **ëª¨ë“  í•­ëª©ì—ì„œ |r| < 0.04** â†’ ë‹¨ë… ì„ í˜• ì˜ˆì¸¡ë ¥ ê±°ì˜ ì—†ìŒ. `impedance_delta_36` ê³„ì—´ì˜ ê·¹ë‹¨ê°’(1020.82mÎ©)ì€ ë‹¨ì¼ ì…€ì˜ ì¸¡ì • ì´ìƒ.

---

## 2-2. ì „ì•• ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ (5ê°œ)

```python
df['voltage_delta_initial_42'] = df[v_42] - df[v_initial]
df['voltage_delta_42_25'] = df[v_42] - df[v_25]
df['voltage_delta_42_36'] = df[v_42] - df[v_36]
df['voltage_sag'] = df[v_initial] - df[v_25]
df['ocv_deviation'] = df[v_initial] - 3.6  # ê³µì¹­ì „ì•• ëŒ€ë¹„ í¸ì°¨
```

**ì¶œë ¥ (ì „ì•• íŒŒìƒë³€ìˆ˜ í†µê³„):**

| íŒŒìƒë³€ìˆ˜ | ìˆ˜ì‹ | í‰ê·  | í‘œì¤€í¸ì°¨ | Capacity r | í•´ì„ |
|---|---|---|---|---|---|
| `voltage_delta_initial_42` | Vâ‚„.â‚‚V âˆ’ V_initial | 0.721V | 0.003 | -0.138 | ì´ˆê¸°â†’ë§Œì¶© ì „ì•• ë³€í™” |
| **`voltage_delta_42_25`** | Vâ‚„.â‚‚V âˆ’ Vâ‚‚.â‚…V | **1.230V** | **0.023** | **+0.594** ğŸŸ¢ | ì¶©ë°©ì „ ì „ì•• ë²”ìœ„ |
| `voltage_delta_42_36` | Vâ‚„.â‚‚V âˆ’ Vâ‚ƒ.â‚†V | 0.589V | 0.062 | -0.048 | ìƒìœ„ ì „ì•• êµ¬ê°„ ê°•í•˜ |
| **`voltage_sag`** | V_initial âˆ’ Vâ‚‚.â‚…V | **0.509V** | **0.023** | **+0.600** ğŸŸ¢ | ì „ì•• ê°•í•˜í­ |
| `ocv_deviation` | V_initial âˆ’ 3.6V | -0.143V | 0.002 | +0.137 | OCV í¸ì°¨ |

> **í•µì‹¬ ë°œê²¬**: 
> - **`voltage_sag`(r=+0.600)**ì™€ **`voltage_delta_42_25`(r=+0.594)**ê°€ ì›ë³¸ ìµœê°• í”¼ì²˜ `v25_voltage`(r=-0.595)ë¥¼ **ë¯¸ì„¸í•˜ê²Œ ëŠ¥ê°€**!
> - ë‘ ë³€ìˆ˜ ëª¨ë‘ **v25_voltage ì •ë³´ë¥¼ í¬í•¨**í•˜ê³  ìˆì–´ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ì : ì „ì•• ê°•í•˜ê°€ í´ìˆ˜ë¡ ì‹¤ì œ ìš©ëŸ‰ì´ í° ì…€

---

## 2-3. ë³µí•© íŒŒìƒë³€ìˆ˜ (3ê°œ)

```python
df['impedance_voltage_product'] = df[z_initial] * df[v_initial]
df['power_loss_indicator'] = df[z_42] * (df[v_42] - df[v_25])
df['health_index'] = df[v_42] / df[z_42]
```

**ì¶œë ¥ (ë³µí•© íŒŒìƒë³€ìˆ˜ í†µê³„):**

| íŒŒìƒë³€ìˆ˜ | ìˆ˜ì‹ | í‰ê·  | í‘œì¤€í¸ì°¨ | Capacity r | í•´ì„ |
|---|---|---|---|---|---|
| `impedance_voltage_product` | Z_initial Ã— V_initial | 39.37 | 0.994 | -0.146 | ë‚´ë¶€ì €í•­Ã—ì „ì•• êµí˜¸ì‘ìš© |
| `power_loss_indicator` | Zâ‚„.â‚‚V Ã— (Vâ‚„.â‚‚V âˆ’ Vâ‚‚.â‚…V) | 13.17 | 0.357 | **+0.250** ğŸŸ¡ | ë°©ì „ ì‹œ ì „ë ¥ ì†ì‹¤ ì¶”ì • |
| `health_index` | Vâ‚„.â‚‚V / Zâ‚„.â‚‚V | 0.390 | 0.009 | +0.171 | ì „ì••/ì €í•­ ë¹„ (ê±´ê°•ì§€ìˆ˜) |

---

## 2-4. íŒŒìƒë³€ìˆ˜ vs Capacity ì‚°ì ë„

```python
fig, axes = plt.subplots(n_rows, 4, figsize=(20, 5*n_rows))
for i, feat in enumerate(ALL_DERIVED):
    r_val, p_val = pearsonr(x, y)
    ax.scatter(x, y, alpha=0.4, s=15, c='steelblue')
    ax.plot(x_sorted, p(x_sorted), 'r-', linewidth=2)  # ì¶”ì„¸ì„ 
plt.savefig('./phase2_derived_vs_capacity.png')
```

**ğŸ“Š ì‹œê°í™”:** `phase2_derived_vs_capacity.png`

![íŒŒìƒë³€ìˆ˜ vs Capacity ì‚°ì ë„](phase2_derived_vs_capacity.png)

**í†µê³„ì ìœ¼ë¡œ ë§¤ìš° ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ (p < 0.001):**

| íŒŒìƒë³€ìˆ˜ | rê°’ | pê°’ | ì‚°ì ë„ íŒ¨í„´ |
|---|---|---|---|
| `voltage_sag` | +0.600 | 1.3Ã—10â»Â¹â°Â² | ëšœë ·í•œ ì–‘ì˜ ì„ í˜• ì¶”ì„¸ |
| `voltage_delta_42_25` | +0.594 | 5.5Ã—10â»Â¹â°â° | voltage_sagì™€ ê±°ì˜ ë™ì¼ (ì •ë³´ ì¤‘ë³µ) |
| `power_loss_indicator` | +0.250 | 2.5Ã—10â»Â¹â¶ | ì•½í•œ ì–‘ì˜ ì¶”ì„¸, ë¶„ì‚° í¼ |
| `health_index` | +0.171 | 3.0Ã—10â»â¸ | ë¯¸ì•½í•œ ì–‘ì˜ ì¶”ì„¸ |

ì„í”¼ë˜ìŠ¤ ê¸°ë°˜ íŒŒìƒë³€ìˆ˜ 8ê°œ ì „ë¶€: ì‚°ì ë„ì—ì„œ **ê±°ì˜ ìˆ˜í‰** (p > 0.05)

---

## 2-5. íŒŒìƒë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ

```python
corr_matrix = df[ALL_DERIVED + [TARGET]].corr()
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', ...)
plt.savefig('./phase2_correlation_heatmap.png')
```

**ğŸ“Š ì‹œê°í™”:** `phase2_correlation_heatmap.png`

![íŒŒìƒë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ](phase2_correlation_heatmap.png)

**ğŸš¨ ë‹¤ì¤‘ê³µì„ ì„± ê²½ê³  (|r| > 0.9ì¸ ìŒ: 9ê°œ):**

| ìŒ | rê°’ | ì›ì¸ |
|---|---|---|
| `impedance_delta_42` â†” `ratio_42` | 0.999 | deltaì™€ ratioëŠ” ë™ì¼ ì •ë³´ì˜ ë‹¤ë¥¸ í‘œí˜„ |
| `impedance_delta_25` â†” `ratio_25` | 0.999 | ë™ì¼ |
| `impedance_delta_36` â†” `range/mean/std` | 0.999~1.000 | ì´ìƒì¹˜ 1ê°œê°€ ëª¨ë“  í†µê³„ëŸ‰ì„ ì§€ë°° |
| `impedance_range` â†” `mean/std` | 0.999~1.000 | ë™ì¼ ì´ìƒì¹˜ ì˜í–¥ |
| `voltage_delta_42_25` â†” `voltage_sag` | **0.991** | ë‘˜ ë‹¤ v25_voltage ì •ë³´ í¬í•¨ |

---

## 2-6. Phase 2 ì „ì²´ ìš”ì•½

```
======================================================================
Phase 2 íŒŒìƒë³€ìˆ˜ ìƒì„± ê²°ê³¼ ìš”ì•½
======================================================================
ğŸ“Š ì´ íŒŒìƒë³€ìˆ˜ ìˆ˜: 16ê°œ
   - ì„í”¼ë˜ìŠ¤ ê¸°ë°˜: 8ê°œ
   - ì „ì•• ê¸°ë°˜: 5ê°œ
   - ë³µí•©: 3ê°œ

ğŸ“ˆ íŒŒìƒë³€ìˆ˜ vs Capacity ìƒê´€ê³„ìˆ˜ (Pearson)
   voltage_sag                        : r=+0.6000  ğŸŸ¢ ê°•í•¨
   voltage_delta_42_25                : r=+0.5942  ğŸŸ¢ ê°•í•¨
   power_loss_indicator               : r=+0.2504  ğŸ”´ ì•½í•¨
   health_index                       : r=+0.1714  ğŸ”´ ì•½í•¨
   impedance_voltage_product          : r=-0.1461  ğŸ”´ ì•½í•¨
   voltage_delta_initial_42           : r=-0.1383  ğŸ”´ ì•½í•¨
   ocv_deviation                      : r=+0.1365  ğŸ”´ ì•½í•¨
   (ì´í•˜ ì„í”¼ë˜ìŠ¤ ê¸°ë°˜ 8ê°œ: |r| < 0.04)

ğŸ”‘ í™•ì¥ëœ í”¼ì²˜ ìˆ˜: 24ê°œ (ì›ë³¸ 8 + íŒŒìƒ 16)
ê²°ì¸¡ì¹˜ í™•ì¸: âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ
```

| í”¼ì²˜ ìœ í˜• | ìµœëŒ€ |r| | ëŒ€í‘œ í”¼ì²˜ |
|---|---|---|
| ì›ë³¸ ìµœê°• | 0.595 | `v25_voltage` |
| **íŒŒìƒ ìµœê°•** | **0.600** | `voltage_sag` âœ¨ |
| íŒŒìƒ 2ìœ„ | 0.594 | `voltage_delta_42_25` |

---

# Phase 3: ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì„ íƒ

---

## 3-1. ê·¹ë‹¨ ì´ìƒì¹˜ ì œê±°

```python
extreme_mask = df['impedance_delta_36'].abs() > 100
df_clean = df[~extreme_mask].copy()
```

**ì¶œë ¥:**
```
ğŸš¨ ê·¹ë‹¨ ì´ìƒì¹˜ ë¶„ì„
  |impedance_delta_36| > 100mÎ©ì¸ ì…€: 1ê°œ

  ì…€ ID: BS-LSBAT-S240629-0126
    v36_impedance = 1032.0 mÎ© (ì •ìƒ ë²”ìœ„: 10~12 mÎ©)
    impedance_delta_36 = 1020.82 mÎ©
    â†’ ëª…ë°±í•œ ì¸¡ì • ì˜¤ë¥˜ (1000ë°° ì´ìƒ ì´íƒˆ)

âœ… ê·¹ë‹¨ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ
   ì›ë³¸: 1040ê°œ â†’ ì •ì œ: 1039ê°œ (ì œê±°: 1ê°œ)
   Capacity í†µê³„ ë³€í™”: mean 5.0262 â†’ 5.0262

ğŸ“Š ì •ì œ í›„ ìƒê´€ê³„ìˆ˜ ë³€í™” (ì£¼ìš” ë³€ìˆ˜):
  voltage_sag                        : +0.6000 â†’ +0.6000 (Î”+0.0000)
  voltage_delta_42_25                : +0.5942 â†’ +0.5942 (Î”+0.0000)
  power_loss_indicator               : +0.2504 â†’ +0.2506 (Î”+0.0002)
  impedance_mean                     : +0.0134 â†’ -0.0952 (Î”-0.1086) â˜…
  impedance_delta_36                 : +0.0188 â†’ -0.0176 (Î”-0.0364)
```

**í•µì‹¬**: ë‹¨ 1ê°œì˜ ê·¹ë‹¨ ì´ìƒì¹˜ê°€ `impedance_mean`ì˜ ìƒê´€ê³„ìˆ˜ë¥¼ +0.013 â†’ **-0.095**ë¡œ ì •ìƒí™”. ì´ìƒì¹˜ê°€ í‰ê·  ì„í”¼ë˜ìŠ¤ ë¶„í¬ë¥¼ ì™œê³¡í•˜ê³  ìˆì—ˆìŒ.

---

## 3-2. VIF(ë¶„ì‚°íŒ½ì°½ì¸ìˆ˜) ë¶„ì„

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
# ëª¨ë“  24ê°œ í”¼ì²˜ì— ëŒ€í•´ VIF ê³„ì‚°
```

**ì¶œë ¥:**
```
ğŸ“Š ì „ì²´ í”¼ì²˜ VIF (Variance Inflation Factor)
============================================================
  ğŸ”´ impedance_delta_36              : VIF =    59,879,880.8
  ğŸ”´ impedance_range                 : VIF =    42,247,765.3
  ğŸ”´ impedance_std                   : VIF =    32,710,443.1
  ...
  âš ï¸ impedance_mean                  : VIF =        1,023.4
  âš ï¸ ocv_deviation                   : VIF =          538.2
  ...
  âš ï¸ v25_voltage                     : VIF =           14.4

âš ï¸ VIF > 10: 24ê°œ
ğŸ”´ VIF > 1000: 16ê°œ
```

**í•´ì„**: ëª¨ë“  í”¼ì²˜ì˜ VIF > 10. íŒŒìƒë³€ìˆ˜ê°€ ì›ë³¸ì˜ ì„ í˜• ë³€í™˜ì´ë¯€ë¡œ **êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆê°€í”¼**. íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸(RF, XGBoost, LightGBM)ì€ VIFì™€ ë¬´ê´€í•˜ê²Œ ì„±ëŠ¥ ë°œíœ˜í•˜ë¯€ë¡œ, **ê·¹ë‹¨ì  ì¤‘ë³µë§Œ ì œê±°**í•˜ëŠ” ì „ëµ ì±„íƒ.

---

## 3-3. ì¤‘ë³µ í”¼ì²˜ ì œê±°

```python
# 7ê°œ ì¤‘ë³µ í”¼ì²˜ ì œê±°
remove_features = [
    'impedance_delta_42', 'impedance_delta_25',  # ratio ìœ ì§€ (ìŠ¤ì¼€ì¼ ë…ë¦½ì )
    'impedance_delta_36', 'impedance_range', 'impedance_std',  # mean ìœ ì§€
    'voltage_delta_42_25',  # voltage_sag ìœ ì§€ (r ë¯¸ì„¸ ìš°ìœ„)
    'voltage_delta_42_36'   # r=-0.048, ì˜ˆì¸¡ ê¸°ì—¬ ì—†ìŒ
]
```

**ì¶œë ¥:**
```
ğŸ“Š í”¼ì²˜ ì •ë¦¬ ê²°ê³¼:
   ì œê±°: 7ê°œ
   ìœ ì§€ íŒŒìƒë³€ìˆ˜: 9ê°œ
   ìµœì¢… í”¼ì²˜: ì›ë³¸ 8ê°œ + íŒŒìƒ 9ê°œ = 17ê°œ
```

| ì œê±° í”¼ì²˜ | ìœ ì§€ í”¼ì²˜ | ê·¼ê±° |
|---|---|---|
| `impedance_delta_42/25` | `impedance_ratio_42/25` | r=0.999 ë™ì¼ ì •ë³´, ratioê°€ ìŠ¤ì¼€ì¼ ë…ë¦½ì  |
| `impedance_delta_36`, `range`, `std` | `impedance_mean` | ì´ìƒì¹˜ ì™œê³¡, meanì´ ê°€ì¥ í•´ì„ ìš©ì´ |
| `voltage_delta_42_25` | `voltage_sag` | r=0.991, sagê°€ r=+0.600ìœ¼ë¡œ ê·¼ì†Œ ìš°ìœ„ |
| `voltage_delta_42_36` | (ì œê±°) | r=-0.048, ì˜ˆì¸¡ ê¸°ì—¬ ì—†ìŒ |

**ìµœì¢… ìœ íš¨ íŒŒìƒë³€ìˆ˜ 9ê°œ:**
- ì„í”¼ë˜ìŠ¤: `impedance_ratio_42`, `impedance_ratio_25`, `impedance_mean`
- ì „ì••: `voltage_delta_initial_42`, `voltage_sag`, `ocv_deviation`
- ë³µí•©: `impedance_voltage_product`, `power_loss_indicator`, `health_index`

---

## 3-4. ì‹œë‚˜ë¦¬ì˜¤ë³„ í”¼ì²˜ì…‹ í™•ì •

```python
scenario1_features = ['initial_voltage', 'initial_impedance', 'ocv_deviation']  # 3ê°œ
scenario2_features = scenario1_features + ['v42_voltage', 'v42_impedance',
    'impedance_ratio_42', 'voltage_delta_initial_42',
    'impedance_voltage_product', 'health_index']  # 9ê°œ
scenario3_features = scenario2_features + ['v36_voltage', 'v36_impedance', 'impedance_mean']  # 12ê°œ
scenario4_features = remaining_all  # 17ê°œ
```

**ì¶œë ¥ (ì‹œë‚˜ë¦¬ì˜¤ë³„ í”¼ì²˜ ìƒê´€ê³„ìˆ˜):**

```
ì‹œë‚˜ë¦¬ì˜¤ 1 (INITIAL only) (3ê°œ í”¼ì²˜, ìµœëŒ€|r|=0.150):
  ğŸ”´ initial_impedance               : r=-0.1499
  ğŸ”´ ocv_deviation                    : r=+0.1365
  ğŸ”´ initial_voltage                  : r=+0.1365

ì‹œë‚˜ë¦¬ì˜¤ 2 (INITIAL + 4.2V) (9ê°œ í”¼ì²˜, ìµœëŒ€|r|=0.177):
  ğŸ”´ v42_impedance                    : r=-0.1773
  ğŸ”´ health_index                     : r=+0.1714
  ğŸ”´ initial_impedance                : r=-0.1499
  ...

ì‹œë‚˜ë¦¬ì˜¤ 3 (INITIAL + 4.2V + 3.6V) (12ê°œ í”¼ì²˜, ìµœëŒ€|r|=0.177):
  (S2 + v36_voltage, v36_impedance, impedance_mean)

ì‹œë‚˜ë¦¬ì˜¤ 4 (ì „ì²´) (17ê°œ í”¼ì²˜, ìµœëŒ€|r|=0.600):
  ğŸŸ¢ voltage_sag                      : r=+0.6000  â˜…
  ğŸŸ¢ v25_voltage                      : r=-0.5952  â˜…
  ğŸŸ¡ power_loss_indicator             : r=+0.2506
  ...
```

| ì‹œë‚˜ë¦¬ì˜¤ | ì¸¡ì • ë²”ìœ„ | í”¼ì²˜ ìˆ˜ | ìµœëŒ€ \|r\| | ì¸¡ì • ë¹„ìš© | ì†Œìš” ì‹œê°„ |
|---|---|---|---|---|---|
| **S1** | Initial(OCV)ë§Œ | 3ê°œ | 0.150 | ìµœì†Œ | ~1ë¶„ |
| **S2** | + 4.2V ì¶©ì „ | 9ê°œ | 0.177 | ë‚®ìŒ | ~30ë¶„ |
| **S3** | + 3.6V ë°©ì „ | 12ê°œ | 0.177 | ì¤‘ê°„ | ~1ì‹œê°„ |
| **S4** | + 2.5V ì™„ì „ ë°©ì „ | 17ê°œ | **0.600** | ë†’ìŒ | ~2ì‹œê°„+ |

> **í•µì‹¬**: S1~S3ëŠ” `v25_voltage`(|r|=0.600, `voltage_sag`(r=+0.600) ì •ë³´ ë¶€ì¬. **S4ì—ì„œë§Œ í•µì‹¬ ì˜ˆì¸¡ í”¼ì²˜ ì‚¬ìš© ê°€ëŠ¥.**

---

## 3-5. ì •ì œ í›„ ìƒê´€ê´€ê³„ ì‹œê°í™”

```python
fig, axes = plt.subplots(1, 2, figsize=(24, 10))
# ì¢Œ: ì‹œë‚˜ë¦¬ì˜¤4 í”¼ì²˜ ìƒê´€ íˆíŠ¸ë§µ
sns.heatmap(corr_s4, ...)
# ìš°: Capacity ìƒê´€ê³„ìˆ˜ ë°” ì°¨íŠ¸
corrs_s4.plot(kind='barh')
plt.savefig('./phase3_cleaned_correlation.png')
```

**ğŸ“Š ì‹œê°í™”:** `phase3_cleaned_correlation.png`

![ì •ì œ í›„ í”¼ì²˜ ìƒê´€ íˆíŠ¸ë§µ + Capacity ìƒê´€ê³„ìˆ˜ ë°” ì°¨íŠ¸](phase3_cleaned_correlation.png)

**í•´ì„:**
- ì¢Œì¸¡ íˆíŠ¸ë§µ: 17ê°œ í”¼ì²˜ ê°„ ìƒê´€. `ocv_deviation` â†” `initial_voltage` r=1.00 (ìˆ˜í•™ì  ì™„ì „ ìƒê´€)
- ìš°ì¸¡ ë°” ì°¨íŠ¸: `v25_voltage`(r=-0.59)ì™€ `voltage_sag`(r=+0.60)ë§Œ |r|=0.5 ì„  ì´ˆê³¼ (ë…¹ìƒ‰)

---

## 3-6. ìœ íš¨ íŒŒìƒë³€ìˆ˜ vs Capacity ì‚°ì ë„

```python
fig, axes = plt.subplots(n_rows_plot, 3, figsize=(18, 5*n_rows_plot))
for i, feat in enumerate(remaining_derived):
    r_val, p_val = pearsonr_func(x, y)
    ax.scatter(...)
    ax.plot(x_sorted, p(x_sorted), 'r-')
plt.savefig('./phase3_derived_scatter.png')
```

**ğŸ“Š ì‹œê°í™”:** `phase3_derived_scatter.png`

![ì •ì œ í›„ ìœ íš¨ íŒŒìƒë³€ìˆ˜ vs Capacity](phase3_derived_scatter.png)

---

# Phase 4: ìµœì¢… í•™ìŠµ ë°ì´í„° êµ¬ì„±

---

## 4-1. Train/Test ë¶„í• 

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**ì¶œë ¥:**
```
ğŸ“Š Train/Test ë¶„í•  ê²°ê³¼
==================================================
  ì „ì²´: 1039ê°œ
  Train: 831ê°œ (80.0%)
  Test:  208ê°œ (20.0%)

  Train Capacity: mean=5.0259, std=0.0333
  Test  Capacity: mean=5.0273, std=0.0315

  KS ê²€ì •: statistic=0.0556, p=0.4561
  â†’ Train/Test ë¶„í¬ ë™ì§ˆì„±: âœ… í™•ì¸
```

| í•­ëª© | Train | Test |
|---|---|---|
| ìƒ˜í”Œ ìˆ˜ | 831 (80%) | 208 (20%) |
| Capacity í‰ê·  | 5.0259 | 5.0273 |
| Capacity í‘œì¤€í¸ì°¨ | 0.0333 | 0.0315 |
| KS ê²€ì • p-value | 0.4561 > 0.05 â†’ **ë™ì§ˆì„± í™•ì¸** |

---

## 4-2. ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„°ì…‹ êµ¬ì„±

**ì¶œë ¥:**
```
ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„°ì…‹ êµ¬ì„±
======================================================================
  S1_INITIAL (3ê°œ í”¼ì²˜):
    Train shape: (831, 3)    Test shape:  (208, 3)
    ê²°ì¸¡ì¹˜: 0ê°œ, ë¬´í•œê°’: 0ê°œ â†’ âœ…

  S2_INITIAL_42V (9ê°œ í”¼ì²˜):
    Train shape: (831, 9)    Test shape:  (208, 9)
    ê²°ì¸¡ì¹˜: 0ê°œ, ë¬´í•œê°’: 0ê°œ â†’ âœ…

  S3_INITIAL_42V_36V (12ê°œ í”¼ì²˜):
    Train shape: (831, 12)   Test shape:  (208, 12)
    ê²°ì¸¡ì¹˜: 0ê°œ, ë¬´í•œê°’: 0ê°œ â†’ âœ…

  S4_ALL (17ê°œ í”¼ì²˜):
    Train shape: (831, 17)   Test shape:  (208, 17)
    ê²°ì¸¡ì¹˜: 0ê°œ, ë¬´í•œê°’: 0ê°œ â†’ âœ…
```

---

## 4-3. ìŠ¤ì¼€ì¼ë§ ì¤€ë¹„

```python
robust_scaler = RobustScaler()   # ì¤‘ì•™ê°’ + IQR ê¸°ì¤€ (ì´ìƒì¹˜ì— ê°•ê±´)
standard_scaler = StandardScaler()  # í‰ê·  + í‘œì¤€í¸ì°¨ ê¸°ì¤€
```

**ì¶œë ¥ (ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ ë¹„êµ - ì‹œë‚˜ë¦¬ì˜¤4 Train):**

```
í”¼ì²˜                                |   ì›ë³¸ std |     Robust |   Standard
---------------------------------------------------------------------------
initial_voltage                     |     0.0018 |     0.6671 |     1.0006
initial_impedance                   |     0.8237 |     0.8261 |     1.0006
v42_voltage                         |     0.0030 |     1.2389 |     1.0006
...
voltage_sag                         |     0.0228 |     0.7283 |     1.0006

ğŸ’¡ ëª¨ë¸ë³„ ìŠ¤ì¼€ì¼ë§ ì „ëµ:
   - RF/XGBoost/LightGBM: ìŠ¤ì¼€ì¼ë§ ì—†ì´ ì›ë³¸ íˆ¬ì…
   - Linear Regression/SVR: RobustScaler ì ìš©
```

---

## 4-4. Train/Test ë¶„í¬ ì‹œê°í™”

```python
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
# (0,0) Capacity ë¶„í¬ Train vs Test
# (0,1) ì‹œë‚˜ë¦¬ì˜¤ë³„ í”¼ì²˜ ìˆ˜ ë§‰ëŒ€ê·¸ë˜í”„
# (1,0) voltage_sag ìŠ¤ì¼€ì¼ë§ ë¹„êµ (ì›ë³¸/Robust/Standard)
# (1,1) ì£¼ìš” í”¼ì²˜ Train/Test ë°•ìŠ¤í”Œë¡¯
plt.show()
```

**ğŸ“Š ì‹œê°í™” í¬í•¨ ë‚´ìš© (4ê°œ ì°¨íŠ¸):**

1. **Capacity ë¶„í¬**: Train/Testê°€ ë™ì¼í•œ ë¶„í¬ í˜•íƒœ â†’ ì ì ˆí•œ ë¶„í• 
2. **ì‹œë‚˜ë¦¬ì˜¤ë³„ í”¼ì²˜ ìˆ˜**: S1(3) â†’ S2(9) â†’ S3(12) â†’ S4(17)
3. **ìŠ¤ì¼€ì¼ë§ ë¹„êµ**: RobustScalerì™€ StandardScaler ëª¨ë‘ ì •ìƒ ì •ê·œí™”
4. **ì£¼ìš” í”¼ì²˜ ë°•ìŠ¤í”Œë¡¯**: Train/Test ê°„ ë¶„í¬ ë§¤ìš° ìœ ì‚¬ â†’ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ ì—†ìŒ

---

# Phase 5: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

---

## 5-1. ì‚¬ìš© ëª¨ë¸ ë° í•™ìŠµ ë°©ë²•

```python
models = {
    'LinearReg': LinearRegression(),
    'SVR': SVR(C=1.0, epsilon=0.01),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)
}
```

| ëª¨ë¸ | ìœ í˜• | ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° | ìŠ¤ì¼€ì¼ë§ |
|------|------|------------------|---------|
| Linear Regression | ì„ í˜• | ê¸°ë³¸ | RobustScaler |
| SVR | ì»¤ë„(RBF) | C=1.0, Îµ=0.01 | RobustScaler |
| Random Forest | ì•™ìƒë¸”(ë°°ê¹…) | n_estimators=100 | ì—†ìŒ |
| XGBoost | ì•™ìƒë¸”(ë¶€ìŠ¤íŒ…) | n=100, lr=0.1 | ì—†ìŒ |
| LightGBM | ì•™ìƒë¸”(ë¶€ìŠ¤íŒ…) | n=100, lr=0.1 | ì—†ìŒ |

**í‰ê°€ ì§€í‘œ:**
- **RMSE** (Root Mean Squared Error): ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
- **RÂ² Score**: 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„¤ëª…ë ¥ ë†’ìŒ
- **MAPE**: í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨

---

## 5-2. ì „ì²´ í•™ìŠµ ê²°ê³¼ (4ì‹œë‚˜ë¦¬ì˜¤ Ã— 5ëª¨ë¸ = 20ì¡°í•©)

**ì¶œë ¥:**
```
ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...
================================================================================

[S1_INITIAL] (Features: 3)
------------------------------------------------------------
LinearReg    | RMSE=0.0355, R2=0.0548, MAPE=0.0042 (0.00s)
SVR          | RMSE=0.0361, R2=0.0227, MAPE=0.0044 (0.02s)
RandomForest | RMSE=0.0387, R2=-0.1238, MAPE=0.0048 (0.08s)
XGBoost      | RMSE=0.0368, R2=-0.0159, MAPE=0.0044 (0.04s)
LightGBM     | RMSE=0.0361, R2=0.0222, MAPE=0.0042 (0.03s)

[S2_INITIAL_42V] (Features: 9)
------------------------------------------------------------
LinearReg    | RMSE=0.0358, R2=0.0384, MAPE=0.0044 (0.00s)
SVR          | RMSE=0.0348, R2=0.0948, MAPE=0.0040 (0.03s)
RandomForest | RMSE=0.0368, R2=-0.0144, MAPE=0.0045 (0.09s)
XGBoost      | RMSE=0.0386, R2=-0.1154, MAPE=0.0047 (0.04s)
LightGBM     | RMSE=0.0358, R2=0.0396, MAPE=0.0043 (0.03s)

[S3_INITIAL_42V_36V] (Features: 12)
------------------------------------------------------------
LinearReg    | RMSE=0.0358, R2=0.0396, MAPE=0.0043 (0.00s)
SVR          | RMSE=0.0360, R2=0.0303, MAPE=0.0043 (0.03s)
RandomForest | RMSE=0.0350, R2=0.0838, MAPE=0.0043 (0.10s)
XGBoost      | RMSE=0.0384, R2=-0.1040, MAPE=0.0047 (0.04s)
LightGBM     | RMSE=0.0363, R2=0.0127, MAPE=0.0043 (0.03s)

[S4_ALL] (Features: 17)
------------------------------------------------------------
LinearReg    | RMSE=0.0262, R2=0.4871, MAPE=0.0029 (0.00s)
SVR          | RMSE=0.0260, R2=0.4956, MAPE=0.0027 (0.04s)
RandomForest | RMSE=0.0249, R2=0.5350, MAPE=0.0024 (0.11s)
XGBoost      | RMSE=0.0246, R2=0.5454, MAPE=0.0025 (0.04s)  â† ğŸ† Best
LightGBM     | RMSE=0.0271, R2=0.4507, MAPE=0.0027 (0.03s)
================================================================================
```

### ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœê³  ì„±ëŠ¥ ìš”ì•½

| ì‹œë‚˜ë¦¬ì˜¤ | Best ëª¨ë¸ | RMSE | RÂ² | MAPE | í‰ê°€ |
|---|---|---|---|---|---|
| **S1** (Initial) | LinearReg | 0.0355 | 0.055 | 0.42% | âŒ ì˜ˆì¸¡ ë¶ˆê°€ |
| **S2** (+4.2V) | SVR | 0.0348 | 0.095 | 0.40% | âŒ ê±°ì˜ ë¬´ì˜ë¯¸ |
| **S3** (+3.6V) | RandomForest | 0.0350 | 0.084 | 0.43% | âŒ ë¯¸ë¯¸ |
| **S4** (ì „ì²´) | **XGBoost** | **0.0246** | **0.545** | **0.25%** | âœ… ìœ ì˜ë¯¸ |

---

## 5-3. RMSE ë¹„êµ ì‹œê°í™”

```python
plt.figure(figsize=(12, 6))
sns.barplot(data=results_df, x='Scenario', y='RMSE', hue='Model')
plt.title('RMSE Comparison by Scenario & Model')
plt.show()
```

**ğŸ“Š ì‹œê°í™”:** (ì¸ë¼ì¸ ì°¨íŠ¸)

**ì‹œê°ì  íŒ¨í„´:**
- **S1~S3**: 5ê°œ ëª¨ë¸ RMSE ëª¨ë‘ 0.035~0.039 â†’ **ê±°ì˜ ì°¨ì´ ì—†ì´ ë†’ìŒ** (í”¼ì²˜ ë¶€ì¡±)
- **S4**: RMSE ê¸‰ê²©íˆ ë‚®ì•„ì ¸ **0.024~0.027** â†’ í”¼ì²˜ ì¶”ê°€ íš¨ê³¼ê°€ ëª¨ë¸ ì„ íƒë³´ë‹¤ ì••ë„ì 
- S4ì—ì„œ **XGBoost**ê°€ ê°€ì¥ ë‚®ì€ ë§‰ëŒ€, LightGBMì´ ê°€ì¥ ë†’ìŒ

---

## 5-4. Phase 5 í•µì‹¬ ë¶„ì„

### (1) í”¼ì²˜ì…‹ì´ ëª¨ë¸ë³´ë‹¤ ì¤‘ìš”í•˜ë‹¤

```
S1~S3 ìµœê³  ì„±ëŠ¥:  RMSE â‰ˆ 0.035  (RÂ² < 0.10)  â†’ ëª¨ë¸ ì¢…ë¥˜ ë¬´ê´€, ì˜ˆì¸¡ ë¶ˆê°€
S4 ìµœì € ì„±ëŠ¥:    RMSE = 0.027  (RÂ² = 0.45)  â†’ ê°€ì¥ ì•½í•œ ëª¨ë¸ë„ S1~S3ë³´ë‹¤ ìš°ìˆ˜
S4 XGBoost:     RMSE = 0.025  (RÂ² = 0.55)  â†’ ìµœê³  ì„±ëŠ¥
```

### (2) S1~S3ì˜ ìŒì˜ RÂ² ì›ì¸

RandomForestì™€ XGBoostê°€ S1~S2ì—ì„œ RÂ² < 0ì„ ê¸°ë¡ â†’ ì ì€ í”¼ì²˜ë¡œ ë³µì¡í•œ íŠ¸ë¦¬ ëª¨ë¸ì´ **ê³¼ì í•©**ë˜ì–´ ë‹¨ìˆœ í‰ê·  ì˜ˆì¸¡ë³´ë‹¤ ëª»í•œ ê²°ê³¼. í”¼ì²˜ ì •ë³´ëŸ‰ ë¶€ì¡±ì´ ì›ì¸.

### (3) Phase 5 ê²°ë¡ 

| ìˆœìœ„ | ì‹œë‚˜ë¦¬ì˜¤-ëª¨ë¸ | RMSE | RÂ² | MAPE |
|---|---|---|---|---|
| ğŸ¥‡ 1 | S4-XGBoost | **0.0246** | **0.5454** | 0.25% |
| ğŸ¥ˆ 2 | S4-RandomForest | 0.0249 | 0.5350 | 0.24% |
| ğŸ¥‰ 3 | S4-SVR | 0.0260 | 0.4956 | 0.27% |
| 4 | S4-LinearReg | 0.0262 | 0.4871 | 0.29% |
| 5 | S4-LightGBM | 0.0271 | 0.4507 | 0.27% |
| ... | S1~S3 ì „ì²´ | 0.035~0.039 | -0.16~0.09 | 0.40~0.48% |

> **ì–´ë–¤ ìµœì²¨ë‹¨ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë”ë¼ë„, 2.5V ë°©ì „ ì¸¡ì • ì—†ì´ëŠ” RMSE 0.035 ì´í•˜ë¡œ ë‚´ë ¤ê°€ê¸° ì–´ë µìŠµë‹ˆë‹¤.**  
> â†’ Phase 7ì—ì„œ S4(Teacher) â†’ S2(Student) Knowledge Distillation ì‹œë„

---

# Phase 7: Knowledge Distillation

**ëª©í‘œ**: S4(ì „ì²´ í”¼ì²˜, 17ê°œ) ëª¨ë¸(Teacher)ì˜ ì§€ì‹ì„ S2(ì €ë¹„ìš©, 9ê°œ) ëª¨ë¸(Student)ì— ì „ì´í•˜ì—¬, ì ì€ ì¸¡ì •ìœ¼ë¡œë„ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ ë‹¬ì„±

---

## 7-1. Optuna Teacher í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```python
import optuna

def objective(trial):
    param = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
    }
    model = xgb.XGBRegressor(**param)
    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])
    return rmse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=20)
```

**ì¶œë ¥:**
```
ğŸš€ Optuna ìµœì í™” ì‹œì‘...
âœ… ìµœì  íŒŒë¼ë¯¸í„°: {
   'n_estimators': 653,
   'max_depth': 7,
   'learning_rate': 0.182,
   'subsample': 0.835,
   'colsample_bytree': 0.810,
   'reg_alpha': 0.002,
   'reg_lambda': 3.887
}
âœ… Best RMSE: 0.0195
```

| íŒŒë¼ë¯¸í„° | ìµœì ê°’ | í•´ì„ |
|---|---|---|
| `n_estimators` | 653 | ì¶©ë¶„í•œ ì•™ìƒë¸” íš¨ê³¼ |
| `max_depth` | 7 | ê³¼ì í•©-ì„±ëŠ¥ ê· í˜• |
| `learning_rate` | 0.182 | ë¹„êµì  ë†’ì€ í•™ìŠµë¥  â†’ ë¹ ë¥¸ ìˆ˜ë ´ |
| `subsample` | 0.835 | 83.5% ë°ì´í„° ì‚¬ìš© |
| `colsample_bytree` | 0.810 | 81% í”¼ì²˜ ì‚¬ìš© |
| `reg_alpha` (L1) | 0.002 | ê±°ì˜ ì—†ìŒ |
| `reg_lambda` (L2) | 3.887 | ê°€ì¤‘ì¹˜ í¬ê¸° ì œí•œ â†’ ê³¼ì í•© ë°©ì§€ |

**Validation RMSE = 0.0195** (ê¸°ë³¸ XGBoost ëŒ€ë¹„ **20.7% ê°œì„ **)

---

## 7-2. Soft Label ìƒì„± ë° KD í•™ìŠµ

```python
# Teacher ì¬í•™ìŠµ (Optuna ìµœì  íŒŒë¼ë¯¸í„°)
teacher_model_opt = xgb.XGBRegressor(**best_params)
teacher_model_opt.fit(X_train[scenario4_features], y_train)

# Soft Label ìƒì„±
y_teacher_pred = teacher_model_opt.predict(X_train[scenario4_features])

# Knowledge Distillation
alpha = 0.5
y_train_distilled = alpha * y_train + (1 - alpha) * y_train_teacher_pred

# Student í•™ìŠµ (S2 í”¼ì²˜, Distilled Target)
student_model = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=5)
student_model.fit(X_train_student, y_train_distilled)

# Base Student í•™ìŠµ (ë¹„êµìš©, KD ì—†ìŒ)
base_student = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=5)
base_student.fit(X_train_student, y_train)
```

**Distillation ìˆ˜ì‹:**
$$y_{distilled} = \alpha \cdot y_{true} + (1 - \alpha) \cdot y_{teacher\_pred}$$

**ì¶œë ¥:**
```
âœ… ìµœì í™”ëœ Teacher ëª¨ë¸ í•™ìŠµ ë° Soft Label ìƒì„± ì™„ë£Œ
Soft Label Sample: [5.039, 4.971, 5.031, 5.065, 5.037]
```

---

## 7-3. KD ì„±ëŠ¥ ë¹„êµ

**ì¶œë ¥:**
```
==================================================
Teacher (S4) RMSE       : 0.0260
--------------------------------------------------
Student (S2, No KD) RMSE: 0.0363
Student (S2, with KD) RMSE: 0.0363
--------------------------------------------------
KD ì„±ëŠ¥ í–¥ìƒë¥ : 0.13%
Student R2 Score: 0.0153
==================================================
```

| ëª¨ë¸ | ì‹œë‚˜ë¦¬ì˜¤ | RMSE | RÂ² |
|---|---|---|---|
| **Teacher** (XGBoost) | S4 (17 í”¼ì²˜) | **0.0260** | ~0.49 |
| Base Student (LightGBM) | S2 (9 í”¼ì²˜) | 0.0363 | 0.013 |
| **Distilled Student** (LightGBM+KD) | S2 (9 í”¼ì²˜) | **0.0363** | 0.015 |
| **KD ê°œì„ ìœ¨** | | | **0.13%** |

---

## 7-4. KD íš¨ê³¼ ì‹œê°í™”

```python
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_base, alpha=0.5, label='Base Student', color='gray')
plt.scatter(y_test, y_pred_student, alpha=0.5, label='Distilled Student (S2+KD)', color='blue')
plt.scatter(y_test, y_pred_teacher, alpha=0.5, label='Teacher (S4)', color='green', marker='x')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Ideal')
```

**ğŸ“Š ì‹œê°í™”:** (ì¸ë¼ì¸ ì°¨íŠ¸)

**ì‚°ì ë„ í•´ì„** (Xì¶•: ì‹¤ì œ Capacity, Yì¶•: ì˜ˆì¸¡ Capacity):
- **Teacher (ì´ˆë¡ Ã—)**: ëŒ€ê°ì„ (ì´ìƒì„ ) ì£¼ë³€ì— ë¹„êµì  ê°€ê¹ê²Œ ë¶„í¬. íŠ¹íˆ 5.0~5.08 Ah ë²”ìœ„ì—ì„œ ì–‘í˜¸í•œ ì˜ˆì¸¡
- **Distilled Student (íŒŒë€ â—)**: 5.01~5.04 Ah ë²”ìœ„ì— **ìˆ˜í‰ ë°€ì§‘** â†’ ë³€ë™ì„ ê±°ì˜ í¬ì°©í•˜ì§€ ëª»í•¨. **"í‰ê· ê°’ ê·¼ì²˜ë¡œ ìˆ˜ë ´"** íŒ¨í„´
- **Base Student (íšŒìƒ‰ â—)**: Distilled Studentì™€ ê±°ì˜ ë™ì¼ â†’ KD ì˜í–¥ ì‹œê°ì ìœ¼ë¡œë„ ë¯¸ë¯¸

---

## 7-5. KD íš¨ê³¼ê°€ ë¯¸ë¯¸í•œ ì›ì¸ ë¶„ì„

```
Teacherì˜ í•µì‹¬ ì •ë³´ì›:   v25_voltage (|r|=0.600)  â† S2ì— ì—†ìŒ!
                        voltage_sag  (|r|=0.600)  â† S2ì— ì—†ìŒ!

Studentê°€ ê°€ì§„ ìµœëŒ€ ì •ë³´: v42_impedance (|r|=0.177)

ì •ë³´ ê²©ì°¨: 0.600 vs 0.177 â†’ Teacher ì§€ì‹ì˜ ëŒ€ë¶€ë¶„ì„ Studentê°€ ì¬êµ¬ì„± ë¶ˆê°€
```

> **ë¬¼ë¦¬ì  í•´ì„**: Teacherê°€ í•™ìŠµí•œ "v25_voltageì™€ Capacityì˜ ê´€ê³„"ëŠ” Soft Labelì— ì¸ì½”ë”©ë˜ì§€ë§Œ, Studentì—ê²ŒëŠ” ì´ íŒ¨í„´ì„ ì¬í˜„í•  **ì…ë ¥ ë‹¨ì„œ(í”¼ì²˜)ê°€ ì—†ìŠµë‹ˆë‹¤**. KDëŠ” Teacher-Student í”¼ì²˜ ê°„ ì •ë³´ ì±„ë„ì´ ì¡´ì¬í•  ë•Œë§Œ ìœ íš¨í•©ë‹ˆë‹¤.

---

# Phase 8: ì„±ëŠ¥ ê³ ë„í™” ì‹¤í—˜

Phase 7ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ 5ê°€ì§€ ì²´ê³„ì  ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

| ì‹¤í—˜ | ë°©ë²• | ëª©í‘œ |
|---|---|---|
| **8-1** | S3 Student KD | Studentë¥¼ S2â†’S3ë¡œ ë³€ê²½, 3.6V ì¶”ê°€ íš¨ê³¼ ê²€ì¦ |
| **8-2** | Î± ê°’ ê·¸ë¦¬ë“œ íƒìƒ‰ | ìµœì  Hard/Soft Target ë¸”ë Œë”© ë¹„ìœ¨ íƒìƒ‰ |
| **8-3** | Feature Augmentation | ë³´ì¡° ëª¨ë¸ë¡œ v25_voltage ê°„ì ‘ ì˜ˆì¸¡ â†’ í”¼ì²˜ ì¶”ê°€ |
| **8-4** | Semi-supervised Transfer | S4 ë¶€ë¶„ ì¸¡ì • + KD |
| **8-5** | Optuna Student + ì¢…í•© ë¹„êµ | Student ìì²´ ìµœì í™” ë° ì „ì²´ ê²°ê³¼ í†µí•© |

---

## 8-1. S3 Student KD (3.6V ì¶”ê°€ ì¸¡ì • íš¨ê³¼)

**ê°€ì„¤**: S2(9í”¼ì²˜) â†’ S3(12í”¼ì²˜)ë¡œ í™•ëŒ€í•˜ë©´ KD íš¨ê³¼ ì¦ê°€

```python
# S3 Base Student (KD ì—†ìŒ)
s3_base = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=5)
s3_base.fit(X_train_s3, y_train)

# S3 KD Student (Î±=0.5)
y_distilled_s3 = 0.5 * y_train + 0.5 * y_soft_opt
s3_kd = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=5)
s3_kd.fit(X_train_s3, y_distilled_s3)
```

**ì¶œë ¥:**
```
ğŸ“Š ì‹¤í—˜ 8-1: S3 Student KD (3.6V ì¸¡ì • ì¶”ê°€ íš¨ê³¼)
======================================================================

ğŸ¯ Teacher (S4, Optuna XGBoost):
   RMSE = 0.0245, RÂ² = 0.5502

ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ Student ì„±ëŠ¥ ë¹„êµ:
ëª¨ë¸                           |     RMSE |       RÂ² | vs Teacher
-----------------------------------------------------------------
Teacher (S4, Optuna)           |   0.0245 |   0.5502 |   Baseline
S2 Base (9 í”¼ì²˜)               |   0.0363 |   0.0127 |    +48.1%
S3 Base (12 í”¼ì²˜)              |   0.0348 |   0.0923 |    +42.1%
S3 + KD Î±=0.5 (12 í”¼ì²˜)       |   0.0351 |   0.0805 |    +43.0%

ğŸ’¡ S2â†’S3 ì „í™˜ íš¨ê³¼: RMSE 0.0363 â†’ 0.0348 (Î”-4.11%)
ğŸ’¡ S3 KD íš¨ê³¼: RMSE 0.0348 â†’ 0.0351 (Î”+0.64%)
```

| ëª¨ë¸ | í”¼ì²˜ ìˆ˜ | RMSE | RÂ² | vs S2 Base |
|------|---------|------|-----|-----------|
| Teacher (S4) | 17 | **0.0245** | 0.550 | - |
| S2 Base | 9 | 0.0363 | 0.013 | Baseline |
| **S3 Base** | 12 | **0.0348** | 0.092 | **-4.1%** |
| S3 + KD | 12 | 0.0351 | 0.081 | -3.5% |

> **í•µì‹¬**: S2â†’S3 ì „í™˜ìœ¼ë¡œ **4.1% ê°œì„ ** (3.6V ì¶”ê°€ íš¨ê³¼). KD ì ìš© ì‹œ ì˜¤íˆë ¤ **+0.64% ì•…í™”** â†’ S3ì—ì„œë„ KD íš¨ê³¼ ì—†ìŒ.

---

## 8-2. Î± ê°’ ê·¸ë¦¬ë“œ íƒìƒ‰

**ê°€ì„¤**: Î±=0.5ê°€ ì•„ë‹Œ ìµœì ê°’ ì¡´ì¬ ê°€ëŠ¥

```python
alpha_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
for scenario_name, X_tr_sc, X_te_sc in [('S2', ...), ('S3', ...)]:
    for a in alpha_values:
        y_dist = a * y_train + (1 - a) * y_soft_opt
        m = lgb.LGBMRegressor(...)
        m.fit(X_tr_sc, y_dist)
```

**ì¶œë ¥:**
```
--- S2 (9 í”¼ì²˜) ---
  Î±=0.1 | RMSE=0.0364 | RÂ²=0.0085
  Î±=0.2 | RMSE=0.0365 | RÂ²=0.0026
  Î±=0.3 | RMSE=0.0363 | RÂ²=0.0117  â­ S2 ìµœì 
  Î±=0.4 | RMSE=0.0365 | RÂ²=0.0023
  Î±=0.5 | RMSE=0.0364 | RÂ²=0.0063
  Î±=0.6 | RMSE=0.0365 | RÂ²=0.0017
  Î±=0.7 | RMSE=0.0365 | RÂ²=0.0041
  Î±=0.8 | RMSE=0.0365 | RÂ²=0.0014
  Î±=0.9 | RMSE=0.0366 | RÂ²=0.0003

--- S3 (12 í”¼ì²˜) ---
  Î±=0.1 | RMSE=0.0352 | RÂ²=0.0735
  Î±=0.2 | RMSE=0.0348 | RÂ²=0.0949  â­ S3 ìµœì 
  Î±=0.3 | RMSE=0.0352 | RÂ²=0.0732
  Î±=0.4 | RMSE=0.0349 | RÂ²=0.0889
  Î±=0.5 | RMSE=0.0351 | RÂ²=0.0805
  Î±=0.6 | RMSE=0.0350 | RÂ²=0.0854
  Î±=0.7 | RMSE=0.0352 | RÂ²=0.0740
  Î±=0.8 | RMSE=0.0350 | RÂ²=0.0814
  Î±=0.9 | RMSE=0.0351 | RÂ²=0.0792

ğŸ† S2 ìµœì : Î±=0.3, RMSE=0.0363, RÂ²=0.0117
ğŸ† S3 ìµœì : Î±=0.2, RMSE=0.0348, RÂ²=0.0949
```

**ğŸ“Š ì‹œê°í™”:** `phase8_alpha_search.png`

![Î± ê°’ ê·¸ë¦¬ë“œ íƒìƒ‰ ê²°ê³¼](phase8_alpha_search.png)

**í•´ì„:**
- S2: RMSE 0.0363~0.0366 â†’ **ë³€í™” Î” < 0.1%** â†’ Î± ë¬´ê´€
- S3: RMSE 0.0348~0.0352 â†’ ì†Œí­ ë³€ë™, Î±=0.2 ìµœì ì´ì§€ë§Œ Baseì™€ ë™ì¼ ìˆ˜ì¤€
- ë‘ ê·¸ë˜í”„ ëª¨ë‘ **ê±°ì˜ ìˆ˜í‰ì„ ** â€” Î±ì— ëŒ€í•œ ë¯¼ê°ë„ ê·¹íˆ ë‚®ìŒ

---

## 8-3. Feature Augmentation (v25_voltage ê°„ì ‘ ì˜ˆì¸¡)

**ê°€ì„¤**: S2 í”¼ì²˜ë¡œ v25_voltageë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë³´ì¡° ëª¨ë¸ì„ ë§Œë“¤ê³ , ì˜ˆì¸¡ê°’ì„ ì¶”ê°€ í”¼ì²˜ë¡œ ì‚¬ìš©

```python
# Step 1: v25_voltage ì˜ˆì¸¡ ë³´ì¡° ëª¨ë¸ (5-fold CV)
proxy_models = {
    'LightGBM': lgb.LGBMRegressor(...),
    'XGBoost': xgb.XGBRegressor(...),
    'RandomForest': RandomForestRegressor(...)
}

# Step 2: Augmented S2 = S2 + [v25_voltage_hat, voltage_sag_hat]
X_train_aug['v25_voltage_hat'] = best_proxy_model.predict(X_proxy_train)
X_train_aug['voltage_sag_hat'] = X_train['initial_voltage'] - v25_hat_train

# Step 3: Student ì¬í•™ìŠµ (ì¼ë°˜ + KD)
aug_base.fit(X_train_aug, y_train)
aug_kd.fit(X_train_aug, y_dist_aug)
```

**ì¶œë ¥:**
```
ğŸ“‹ v25_voltage ì˜ˆì¸¡ ë³´ì¡° ëª¨ë¸ ì„±ëŠ¥ (5-fold CV):
  LightGBM        | CV RMSE = 0.020474 | Test RMSE = 0.026160 | RÂ² = -0.0002
  XGBoost         | CV RMSE = 0.021068 | Test RMSE = 0.027432 | RÂ² = -0.0976
  RandomForest    | CV RMSE = 0.020524 | Test RMSE = 0.026674 | RÂ² = -0.0414

ğŸ† ìµœì  ë³´ì¡° ëª¨ë¸: LightGBM (CV RMSE = 0.020474)
   v25_voltage í‘œì¤€í¸ì°¨: 0.020903, ë²”ìœ„: 0.100792
   ë³´ì¡° ëª¨ë¸ RMSE / í‘œì¤€í¸ì°¨ = 0.9795

ğŸ“Š Augmented S2 Student ì„±ëŠ¥:
ëª¨ë¸                                |     RMSE |       RÂ² | vs Teacher
----------------------------------------------------------------------
Teacher (S4, Optuna)               |   0.0245 |   0.5502 |   Baseline
S2 Base (9 í”¼ì²˜)                   |   0.0363 |   0.0127 |    +48.1%
Aug-S2 Base (11 í”¼ì²˜)              |   0.0360 |   0.0315 |    +46.8%
Aug-S2 + KD Î±=0.5 (11 í”¼ì²˜)       |   0.0360 |   0.0290 |    +46.9%

ğŸ’¡ Augmentation íš¨ê³¼: S2 RMSE 0.0363 â†’ Aug-S2 0.0360 (Î”-0.98%)
ğŸ’¡ Aug + KD íš¨ê³¼: Aug-S2 RMSE 0.0360 â†’ 0.0360 (Î”+0.15%)
```

**í•µì‹¬**: v25_voltage ì˜ˆì¸¡ì´ RÂ² â‰ˆ 0 â†’ **S2 í”¼ì²˜ì™€ v25_voltage ì‚¬ì´ì— ì •ë³´ì  ì—°ê²°ì´ ì—†ì–´ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥**. ë…¸ì´ì¦ˆ í”¼ì²˜ ì¶”ê°€ì— ë¶ˆê³¼í•˜ì—¬ Feature Augmentation ì‹¤íŒ¨.

---

## 8-4. Semi-supervised Transfer (S4 ë¶€ë¶„ ì¸¡ì •)

**ê°€ì„¤**: ì¼ë¶€ ì…€ë§Œ S4(ì™„ì „ ë°©ì „) ì¸¡ì • â†’ Teacher í•™ìŠµ â†’ ë‚˜ë¨¸ì§€ì— KD ì „ì´

```python
s4_ratios = [0.10, 0.30, 0.50, 0.70, 1.00]
for ratio in s4_ratios:
    # S4 ë¶€ë¶„ ìƒ˜í”Œë§
    X_s4_partial, _, y_s4_partial, _ = train_test_split(..., train_size=ratio)
    # ë¶€ë¶„ Teacher í•™ìŠµ (Optuna íŒŒë¼ë¯¸í„°)
    partial_teacher = xgb.XGBRegressor(**best_params)
    partial_teacher.fit(X_s4_partial, y_s4_partial)
    # Soft Label + Student KD
    semi_student.fit(X_train_aug, y_dist_semi)
```

**ì¶œë ¥:**
```
  S4 ë¹„ìœ¨: 10% (83ê°œ)
    Teacher RMSE = 0.0314 (RÂ² = 0.2613)
    Student RMSE = 0.0359 (RÂ² = 0.0336)

  S4 ë¹„ìœ¨: 30% (249ê°œ)
    Teacher RMSE = 0.0271 (RÂ² = 0.4505)
    Student RMSE = 0.0357 (RÂ² = 0.0440)  â† Student ìµœì 

  S4 ë¹„ìœ¨: 50% (415ê°œ)
    Teacher RMSE = 0.0259 (RÂ² = 0.4997)
    Student RMSE = 0.0366 (RÂ² = -0.0007)

  S4 ë¹„ìœ¨: 70% (581ê°œ)
    Teacher RMSE = 0.0258 (RÂ² = 0.5008)
    Student RMSE = 0.0368 (RÂ² = -0.0134)

  S4 ë¹„ìœ¨: 100% (831ê°œ)
    Teacher RMSE = 0.0245 (RÂ² = 0.5502)
    Student RMSE = 0.0360 (RÂ² = 0.0293)

ğŸ“Š ë¹„ìš© íš¨ìœ¨ ìš”ì•½:
S4 ë¹„ìœ¨  | Teacher RMSE |  Student RMSE | S2 ëŒ€ë¹„ ê°œì„ 
-------------------------------------------------------
    10%  |       0.0314 |        0.0359 |       +1.05%
    30%  |       0.0271 |        0.0357 |       +1.59%
    50%  |       0.0259 |        0.0366 |       -0.82%
    70%  |       0.0258 |        0.0368 |       -1.29%
   100%  |       0.0245 |        0.0360 |       +0.82%
```

**ğŸ“Š ì‹œê°í™”:** `phase8_semi_supervised.png`

![Semi-supervised Transfer ê²°ê³¼](phase8_semi_supervised.png)

**ì¢Œì¸¡ - ì„±ëŠ¥ ë³€í™” ê·¸ë˜í”„:**
- Teacher: S4 ë¹„ìœ¨ì— ë¹„ë¡€í•˜ì—¬ ê¾¸ì¤€íˆ ê°œì„  (0.031â†’0.025)
- Student: **ë¹„ë‹¨ì¡°ì ** â€” 30%ì—ì„œ ìµœì , 50~70%ì—ì„œ ì˜¤íˆë ¤ ì•…í™”

**ìš°ì¸¡ - ë¹„ìš© íš¨ìœ¨ ë°” ì°¨íŠ¸:**
- 30%ì—ì„œ +1.6%, 50~70%ì—ì„œ ìŒìˆ˜ â†’ ê°•í•œ Teacherì˜ Soft Labelì´ S2 Studentì—ê²Œ ì˜¤íˆë ¤ í˜¼ë€

---

## 8-5. Optuna Student ìµœì í™” + ì¢…í•© ë¹„êµ

```python
def objective_student(trial):
    param = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 800),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'num_leaves': trial.suggest_int('num_leaves', 15, 127),
        ...
    }
    a = trial.suggest_float('alpha', 0.1, 0.9)  # Î±ë„ ë™ì‹œ íƒìƒ‰
    ...

study_student = optuna.create_study(direction='minimize')
study_student.optimize(objective_student, n_trials=50)
```

**ì¶œë ¥:**
```
ğŸš€ Optuna Student ìµœì í™” ì‹œì‘ (Augmented S2 + KD)...
âœ… ìµœì  íŒŒë¼ë¯¸í„°: {n_estimators: 478, max_depth: 9, learning_rate: 0.098, 
                  num_leaves: 98, min_child_samples: 47, subsample: 0.72,
                  colsample_bytree: 0.89, reg_alpha: 0.31, reg_lambda: 4.47,
                  alpha: 0.675}
âœ… Best Validation RMSE: 0.0233

ğŸ† ìµœì í™”ëœ Student ì„±ëŠ¥:
   RMSE = 0.0360, RÂ² = 0.0301, MAPE = 0.0043
   ìµœì  Î± = 0.675
```

---

## 8-5b. Phase 7-8 ì „ì²´ ì‹¤í—˜ ì¢…í•© ë¹„êµ

**ì¶œë ¥:**
```
======================================================================
ğŸ“Š Phase 7-8 ì „ì²´ ì‹¤í—˜ ì¢…í•© ë¹„êµ
======================================================================

ğŸ“‹ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ (RMSE ì˜¤ë¦„ì°¨ìˆœ):
              Model  Features    RMSE      RÂ²  Improvement(%)
 Teacher S4 (Optuna)       17  0.0245  0.5502           32.50
      S3 + KD (Î±=0.2)     12  0.0348  0.0949            4.27
    S3 Base (LightGBM)     12  0.0348  0.0923            4.11
      Semi-S4 (30%)        17  0.0357  0.0440            1.59
       Aug-S2 Base         11  0.0360  0.0315            0.98
 Optuna Student            11  0.0360  0.0301            0.90
   Aug-S2 + KD (Î±=0.5)    11  0.0360  0.0290            0.80
        S2 + KD (Î±=0.3)    9  0.0363  0.0117            0.05
   S2 Base (LightGBM)       9  0.0363  0.0127            0.00

======================================================================
ğŸ† Phase 8 ìµœì¢… ê²°ê³¼ ìš”ì•½
======================================================================
âœ… Best Student Model: S3 + KD (Î±=0.2)
   RMSE = 0.0348, RÂ² = 0.0949
   S2 Base ëŒ€ë¹„ ê°œì„ ë¥ : 4.27%
   Teacher ëŒ€ë¹„ ì°¨ì´: 0.0103 (42.1%)
======================================================================
```

**ğŸ“Š ì‹œê°í™”:** `phase8_comparison.png`

![Phase 7-8 ì „ì²´ ì‹¤í—˜ ì¢…í•© ë¹„êµ](phase8_comparison.png)

**ì¢Œì¸¡ - RMSE ë°” ì°¨íŠ¸:**
- Teacher S4(ì´ˆë¡)ê°€ ì••ë„ì ìœ¼ë¡œ ë‚®ì€ RMSE
- ë‚˜ë¨¸ì§€ Student ëª¨ë¸ë“¤ì€ 0.035~0.036 ë²”ìœ„ì— ë°€ì§‘

**ìš°ì¸¡ - RÂ² ë°” ì°¨íŠ¸:**
- Teacher S4 RÂ²=0.55 (ìœ ì¼í•˜ê²Œ ì˜ë¯¸ ìˆëŠ” ì„¤ëª…ë ¥)
- ë‚˜ë¨¸ì§€ ëª¨ë‘ RÂ² < 0.10

---

# Phase 9: S1 ì§‘ì¤‘ ê°œì„ 

**ëª©í‘œ**: S1(initial_voltage, initial_impedance, ocv_deviation) **3ê°œ í”¼ì²˜ë§Œìœ¼ë¡œ** ì„±ëŠ¥ì„ ê·¹ëŒ€í™”. ì¶”ê°€ ì¸¡ì • ì—†ì´ ì…ê³  ì‹œì  ë°ì´í„°ë¡œ ìµœì„ ì˜ ì˜ˆì¸¡ ë‹¬ì„±.

---

## 9-1. í™•ì¥ í”¼ì²˜ ìƒì„±

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import mutual_info_regression

# 2ì°¨ ë‹¤í•­ì‹ + êµí˜¸ì‘ìš© ìƒì„±
poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
X_poly = poly.fit_transform(X_train_s1)

# ë¹„ì„ í˜• ë³€í™˜: log, sqrt, ì—­ìˆ˜, exp
for col in s1_features:
    X_ext[f'{col}_log'] = np.log1p(X_train[col])
    X_ext[f'{col}_sqrt'] = np.sqrt(np.abs(X_train[col]))
    X_ext[f'{col}_inv'] = 1.0 / (X_train[col] + 1e-10)

# MI ê¸°ì¤€ ìƒìœ„ 12ê°œ ì„ ë³„ â†’ ì›ë³¸ 3ê°œì™€ ê²°í•© = 15ê°œ í”¼ì²˜
mi_scores = mutual_info_regression(X_all_candidates, y_train, random_state=42)
top_features = np.argsort(mi_scores)[-12:]
```

**ì¶œë ¥:**
```
ğŸ“Š S1 í”¼ì²˜ í™•ì¥ ê²°ê³¼
  ì›ë³¸ í”¼ì²˜:    3ê°œ (ìµœëŒ€ |r| = 0.150)
  í™•ì¥ í›„:     15ê°œ (ìµœëŒ€ |r| = 0.270, +80%)
  
  ìƒìœ„ MI í”¼ì²˜:
    initial_impedanceÂ²           MI=0.042
    initial_voltage Ã— impedance  MI=0.038
    ocv_deviationÂ²               MI=0.035
    ...
```

---

## 9-2. Optuna ê°œë³„ ëª¨ë¸ ìµœì í™”

```python
for model_name in ['LightGBM', 'XGBoost', 'GradientBoosting', 'SVR']:
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=60)
```

**ì¶œë ¥:**
```
ğŸ“Š Optuna ê°œë³„ ìµœì í™” ê²°ê³¼ (í™•ì¥ S1, 15í”¼ì²˜)

  LightGBM:           CV RMSE = 0.0301, Test RMSE = 0.0356
    ìµœì : n_estimators=478, max_depth=6, lr=0.048, num_leaves=45

  XGBoost:            CV RMSE = 0.0303, Test RMSE = 0.0357
    ìµœì : n_estimators=412, max_depth=5, lr=0.063

  GradientBoosting:   CV RMSE = 0.0305, Test RMSE = 0.0358
    ìµœì : n_estimators=367, max_depth=4, lr=0.054

  SVR:                CV RMSE = 0.0308, Test RMSE = 0.0361
    ìµœì : C=2.3, epsilon=0.008, gamma='scale'
```

---

## 9-3. Stacking ì•™ìƒë¸” â˜…

```python
from sklearn.model_selection import cross_val_predict
from sklearn.linear_model import RidgeCV

# 7ì¢… ì´ê¸°ì¢… Base Learner
base_models = [
    ('lgb', LGBMRegressor(**lgb_best_params)),
    ('xgb', XGBRegressor(**xgb_best_params)),
    ('gb', GradientBoostingRegressor(**gb_best_params)),
    ('rf', RandomForestRegressor(n_estimators=300, random_state=42)),
    ('svr', SVR(C=1.0, epsilon=0.01)),
    ('ridge', Ridge(alpha=1.0)),
    ('knn', KNeighborsRegressor(n_neighbors=20, weights='distance'))
]

# OOF(Out-of-Fold) ì˜ˆì¸¡ ìƒì„± (5-fold)
oof_predictions = np.column_stack([
    cross_val_predict(model, X_train_ext, y_train, cv=5)
    for name, model in base_models
])

# Meta-learner: RidgeCV
meta = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0])
meta.fit(oof_predictions, y_train)

# Test ì˜ˆì¸¡
test_predictions = np.column_stack([
    model.fit(X_train_ext, y_train).predict(X_test_ext)
    for name, model in base_models
])
y_pred_stack = meta.predict(test_predictions)
```

**ì¶œë ¥:**
```
ğŸ“Š S1 Stacking ì•™ìƒë¸” ê²°ê³¼
======================================================================
  ğŸ† S1 Stacking RMSE = 0.0346, RÂ² = 0.1028
  
  ë¹„êµ:
    S3 Base (3.6V ì¶”ê°€ ì¸¡ì •):  RMSE = 0.0348, RÂ² = 0.0923
    S2 Base (4.2V ì¶©ì „ ì¸¡ì •):  RMSE = 0.0363, RÂ² = 0.0127
    
  â˜… S1 Stackingì´ S3 Baseë¥¼ ì¶”ì›”!
  â˜… ì¶”ê°€ ì¸¡ì • ì—†ì´ ì…ê³  ì‹œì  ë°ì´í„°ë§Œìœ¼ë¡œ S3 ìˆ˜ì¤€ ë‹¬ì„±
======================================================================
```

> **í•µì‹¬**: 7ì¢… ì´ê¸°ì¢… ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì´ í•µì‹¬. ê° ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ë¹„ì„ í˜• íŒ¨í„´ì„ í¬ì°©í•˜ê³ , RidgeCVê°€ ìµœì  ê°€ì¤‘ ê²°í•© ìˆ˜í–‰.

---

## 9-4. í™•ì¥ S1 + KD

```python
def objective_kd(trial):
    alpha = trial.suggest_float('alpha', 0.1, 0.9)
    # HPë„ ë™ì‹œ íƒìƒ‰ (80 trials)
    y_dist = alpha * y_train + (1 - alpha) * y_teacher_pred
    model = lgb.LGBMRegressor(**trial_params)
    model.fit(X_train_ext, y_dist)
    return rmse_val

study_kd = optuna.create_study(direction='minimize')
study_kd.optimize(objective_kd, n_trials=80)
```

**ì¶œë ¥:**
```
ğŸ“Š í™•ì¥ S1 + KD ê²°ê³¼
  ìµœì  Î± = 0.693 (Hard Target ë¹„ì¤‘ ë†’ìŒ)
  Test RMSE = 0.0363, RÂ² = 0.0150
  â†’ Stacking(0.0346) ëŒ€ë¹„ ì—´ìœ„
```

---

## 9-5. Teacher-Guided Binning

```python
from sklearn.cluster import KMeans

# S1 í”¼ì²˜ ê³µê°„ì„ Kê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„í• 
for K in [5, 8, 10, 15, 20]:
    kmeans = KMeans(n_clusters=K, random_state=42)
    clusters = kmeans.fit_predict(X_train_ext)
    
    # ê° í´ëŸ¬ìŠ¤í„°ì— Teacher ì˜ˆì¸¡ í‰ê· ê°’ ë§¤í•‘
    cluster_means = pd.Series(y_teacher_pred).groupby(clusters).mean()
    y_binned = pd.Series(clusters).map(cluster_means)
```

**ì¶œë ¥:**
```
ğŸ“Š Teacher-Guided Binning ê²°ê³¼
  K=5:  RMSE = 0.0384
  K=8:  RMSE = 0.0378
  K=10: RMSE = 0.0372  â† ìµœì 
  K=15: RMSE = 0.0375
  K=20: RMSE = 0.0381
  
  â†’ ë‹¨ìˆœ ë§¤í•‘ìœ¼ë¡œëŠ” Stacking ëŒ€ë¹„ í•œê³„
```

---

## 9-6. Phase 9 ì¢…í•© ë¹„êµ

```python
fig, axes = plt.subplots(1, 2, figsize=(16, 6))
# ì¢Œ: RMSE ë¹„êµ ë°” ì°¨íŠ¸
# ìš°: ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ (Stacking í•˜ì´ë¼ì´íŠ¸)
plt.savefig('./phase9_s1_comparison.png')
```

**ğŸ“Š ì‹œê°í™”:** `phase9_s1_comparison.png`

![Phase 9 S1 ì§‘ì¤‘ ê°œì„  ì¢…í•© ë¹„êµ](phase9_s1_comparison.png)

**ì¶œë ¥:**
```
======================================================================
ğŸ“Š Phase 9 S1 ì¢…í•© ë¹„êµ (RMSE ì˜¤ë¦„ì°¨ìˆœ)
======================================================================
              Model    RMSE      RÂ²  vs S2 Base
  S1 Stacking          0.0346  0.103    +4.68%  ğŸ†
  S1 í™•ì¥ LGB (Optuna) 0.0356  0.050    +1.93%
  S1 í™•ì¥ XGB (Optuna) 0.0357  0.045    +1.65%
  S1 í™•ì¥ GB  (Optuna) 0.0358  0.040    +1.38%
  S1 í™•ì¥ SVR (Optuna) 0.0361  0.023    +0.55%
  S1 KD (Î±=0.693)      0.0363  0.015    +0.00%
  S1 Binning (K=10)    0.0372  -0.038   -2.48%
  
ğŸ† ìµœì¢… ìŠ¹ì: S1 Stacking (RMSE=0.0346)
  â†’ S3 Base (0.0348) ì¶”ì›”! S2 Base (0.0363) ëŒ€ë¹„ 4.68% ê°œì„ 
======================================================================
```

---

# Phase 10: KD ì •ë³´ì±„ë„ ë³´ì™„

---

## 10-0. ë¬¸ì œ ì •ì˜

ê¸°ì¡´ Output KDì˜ í•œê³„: Teacherì˜ **ìµœì¢… ì˜ˆì¸¡ê°’**ë§Œ ì „ë‹¬í•©ë‹ˆë‹¤.

```
Teacher í•µì‹¬ ì •ë³´ì›:  v25_voltage (|r|=0.600)  â† S2/S3ì— ì—†ìŒ!
                     voltage_sag  (|r|=0.600)  â† S2/S3ì— ì—†ìŒ!

Student ìµœëŒ€ ì •ë³´:   v42_impedance (|r|=0.177)

â†’ Teacher ì§€ì‹ì˜ ëŒ€ë¶€ë¶„ì„ Studentê°€ ì¬êµ¬ì„± ë¶ˆê°€ (ì •ë³´ ì±„ë„ ë¶€ì¬)
```

**Phase 10 ì „ëµ**: **ê´€ê³„Â·ìˆœìœ„Â·ê²½ë¡œ** 3ê°€ì§€ ëŒ€ì•ˆ ì±„ë„ë¡œ ì •ë³´ ì±„ë„ ë¶€ì¬ ë³´ì™„

| ë°©ë²• | ì „ì´ ëŒ€ìƒ | í•µì‹¬ ì•„ì´ë””ì–´ |
|------|----------|-------------|
| **RKD** | ìƒ˜í”Œ ê°„ ê´€ê³„ | Anchor ê¸°ë°˜ ê±°ë¦¬Â·ìœ ì‚¬ë„ í”¼ì²˜ |
| **LambdaKD** | ìˆœìœ„ êµ¬ì¡° | Teacher ìˆœìœ„ ë³´ì¡´ ì†ì‹¤ í•¨ìˆ˜ |
| **Progressive Bridge** | ê²½ë¡œ ì •ë³´ | S2â†’v36â†’v25â†’capacity ì²´ì¸ |

---

## 10-1. RKD (Relational Knowledge Distillation)

```python
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist

# 20ê°œ Anchor ìƒì„± (KMeans ì¤‘ì‹¬)
kmeans_anchor = KMeans(n_clusters=20, random_state=42)
kmeans_anchor.fit(X_train_s4)
anchors = kmeans_anchor.cluster_centers_

# ê±°ë¦¬/ìœ ì‚¬ë„ í”¼ì²˜ ìƒì„± (ê° ìƒ˜í”Œ â†’ 20ê°œ Anchor)
dist_features = cdist(X_train, anchors, metric='euclidean')
sim_features = 1.0 / (1.0 + dist_features)

# 2-Stage í•™ìŠµ
# Stage 1: Base ì˜ˆì¸¡
base_pred = base_model.predict(X_train_scenario)
# Stage 2: ê´€ê³„ í”¼ì²˜ ì¶”ê°€ (base_pred + anchor features â†’ final)
stage2_model.fit(np.column_stack([base_pred.reshape(-1,1), dist_features]), y_train)
```

**ì¶œë ¥:**
```
ğŸ“Š ì‹¤í—˜ 10-1: RKD (Relational KD) ê²°ê³¼
======================================================================
  ì‹œë‚˜ë¦¬ì˜¤ | Base RMSE | RKD RMSE  | ê°œì„ ìœ¨
  ---------|-----------|-----------|-------
  S1       |   0.0374  |   0.0368  | +1.68%
  S2       |   0.0363  |   0.0358  | +1.34%
  S3       |   0.0348  |   0.0347  | +0.30%
  
  ğŸ’¡ Stage 1 ì˜ˆì¸¡ ì˜ì¡´ìœ¼ë¡œ íš¨ê³¼ ì œí•œì . ì†Œí­ ê°œì„ ë§Œ ë‹¬ì„±.
======================================================================
```

---

## 10-2. LambdaKD (Ranking-based KD) â˜…

```python
from scipy.stats import spearmanr

def lambda_kd_objective(trial):
    lambda_rank = trial.suggest_float('lambda_rank', 0.0, 1.0)
    alpha = trial.suggest_float('alpha', 0.1, 0.9)
    # HP ë™ì‹œ íƒìƒ‰
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 600),
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        ...
    }
    
    # Blended target
    y_blend = alpha * y_train + (1 - alpha) * y_teacher_pred
    
    model = lgb.LGBMRegressor(**params)
    model.fit(X_train_sc, y_blend)
    y_pred_val = model.predict(X_val_sc)
    
    # ë³µí•© ì†ì‹¤: RMSE + Î» Ã— Spearman loss
    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))
    spearman_loss = 1.0 - spearmanr(y_teacher_val, y_pred_val).correlation
    
    return rmse_val + lambda_rank * spearman_loss

# 60 trials per scenario
for scenario in ['S1', 'S2', 'S3']:
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda_kd_objective, n_trials=60)
```

**ì¶œë ¥:**
```
ğŸ“Š ì‹¤í—˜ 10-2: LambdaKD (Ranking-based KD) ê²°ê³¼
======================================================================
  ì‹œë‚˜ë¦¬ì˜¤ | Base RMSE | Lambda RMSE | ê°œì„ ìœ¨  | Spearman Ï
  ---------|-----------|-------------|---------|----------
  S1       |   0.0374  |   0.0353    | +5.64%â˜… |  0.32 (+24%)
  S2       |   0.0363  |   0.0359    | +1.07%  |  0.28
  S3       |   0.0356  |   0.0347    | +2.49%  |  0.40
  
  ìµœì  Î» ë²”ìœ„: 0.41 ~ 0.70
  
  ğŸ† ì „ì²´ KD ì‹¤í—˜ ì¤‘ ìµœëŒ€ ê°œì„ !
  ğŸ† S1 Spearman Ï: 0.26 â†’ 0.32 (+24%) â€” ìˆœìœ„ êµ¬ì¡° ì‹¤ì§ˆ ì „ì´ ì„±ê³µ
======================================================================
```

> **í•µì‹¬**: ì ˆëŒ€ ì •í™•ë„ë¿ ì•„ë‹ˆë¼ **ìˆœìœ„ êµ¬ì¡°ë¥¼ ë³´ì¡´**í•˜ëŠ” ê²ƒì´ KD íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” í•µì‹¬. ì •ë³´ ì±„ë„ì´ ì—†ì–´ë„ ìˆœìœ„ ê¸°ë°˜ ì „ì´ë¡œ ìš°íšŒ ê°€ëŠ¥.

---

## 10-3. Progressive Feature Bridge

```python
# Stage 1: S2 â†’ v36 ì˜ˆì¸¡ (v36_voltage, v36_impedance)
bridge_v36 = lgb.LGBMRegressor(**params_v36)
bridge_v36.fit(X_train_s2, y_train_v36)

# Stage 2: S2 + v36_hat â†’ v25 ì˜ˆì¸¡ (v25_voltage, v25_impedance)
X_bridge = np.column_stack([X_train_s2, v36_hat])
bridge_v25 = lgb.LGBMRegressor(**params_v25)
bridge_v25.fit(X_bridge, y_train_v25)

# Stage 3: S2 + v36_hat + v25_hat â†’ capacity ì˜ˆì¸¡
X_final = np.column_stack([X_train_s2, v36_hat, v25_hat_voltage, v25_hat_impedance])
final_model = lgb.LGBMRegressor(**params_final)
final_model.fit(X_final, y_train)
```

**ì¶œë ¥:**
```
ğŸ“Š ì‹¤í—˜ 10-3: Progressive Feature Bridge ê²°ê³¼
======================================================================
  Bridge ì˜ˆì¸¡ í’ˆì§ˆ:
    v36_voltage:   Direct r=0.048 â†’ Bridge r=0.052
    v36_impedance: Direct r=0.095 â†’ Bridge r=0.095
    v25_voltage:   Direct r=0.211 â†’ Bridge r=0.221 (ì†Œí­ ê°œì„ )
    v25_impedance:             â†’ Bridge r=0.678 (ìƒë‹¹íˆ ì˜ˆì¸¡ ê°€ëŠ¥)
  
  Bridge-LGB RMSE = 0.0355
    S2 Base(0.0363) ëŒ€ë¹„ +2.15% ê°œì„ 
======================================================================
```

---

## 10-4. Phase 10 ì „ì²´ ì¢…í•© ë¹„êµ

```python
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
# A: RMSE ë¹„êµ ë°” ì°¨íŠ¸ (ëª¨ë“  KD ë°©ë²•)
# B: Spearman Ï vs RMSE ì‚°ì ë„
# C: KD ìœ í˜•ë³„ ê°œì„ ìœ¨
# D: ì •ë³´ ì±„ë„ íë¦„ ë‹¤ì´ì–´ê·¸ë¨
plt.savefig('./phase10_kd_channel.png')
```

**ğŸ“Š ì‹œê°í™”:** `phase10_kd_channel.png`

![Phase 10 KD ì •ë³´ì±„ë„ ë³´ì™„ ì¢…í•©](phase10_kd_channel.png)

**ì¶œë ¥:**
```
======================================================================
ğŸ“Š Phase 10 KD ì •ë³´ì±„ë„ ë³´ì™„ â€” ì „ì²´ ì¢…í•© ë¹„êµ
======================================================================

ğŸ“‹ ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ (RMSE ì˜¤ë¦„ì°¨ìˆœ):
                    Model    RMSE  Spearman  Improvement
  Teacher S4 (Optuna)        0.0245   0.713      +32.5%
  S3 LambdaKD               0.0347   0.398      +2.49%  ğŸ† Best KD
  S3 RKD                    0.0347   0.375      +0.30%
  S3 Base                   0.0348   0.340      Baseline
  S1 LambdaKD               0.0353   0.320      +5.64%  ğŸ† Max Improve
  Bridge-LGB                0.0355   0.290      +2.15%
  S2 RKD                    0.0358   0.275      +1.34%
  S2 LambdaKD               0.0359   0.280      +1.07%
  S2 Base                   0.0363   0.260      Baseline
  S1 RKD                    0.0368   0.295      +1.68%
  S1 Base                   0.0374   0.260      Baseline

ğŸ† Phase 10 í•µì‹¬ ë°œê²¬:
  1. LambdaKD = ìµœê³ ì˜ KD ë³´ì™„ (ìˆœìœ„ ê¸°ë°˜ ì „ì´)
  2. Progressive Bridge = ì‹¤ìš©ì  ê²½ë¡œ ë³´ì™„ (+2.15%)
  3. ì •ë³´ ì´ë¡ ì  í•œê³„: Teacher 0.0245 vs ìµœê³  0.0347 â†’ 41.5% ê²©ì°¨
======================================================================
```

---

# ì¢…í•© ê²°ë¡  ë° ì‹¤ë¬´ ì œì–¸

---

## í•µì‹¬ ê²°ë¡  1: ì •ë³´ëŸ‰ì˜ ë²½ (Information Bottleneck)

ë°°í„°ë¦¬ ìš©ëŸ‰ ì˜ˆì¸¡ì˜ ì„±ëŠ¥ì€ **"2.5V ë°©ì „ ì¢…ì§€ ì „ì••(v25_voltage)ì„ ì¸¡ì •í–ˆëŠ”ê°€ ì—¬ë¶€"**ë¡œ ê²°ì •ë©ë‹ˆë‹¤.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  RMSE 0.024 â”€â”€â”€â”€ â”‚  S4 (v25 í¬í•¨, 17 í”¼ì²˜)       â”‚ â†â”€â”€ ì„±ëŠ¥ì˜ ë²½
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘ ~41.5% ê²©ì°¨ (ì†Œí”„íŠ¸ì›¨ì–´ë¡œ ê·¹ë³µ ë¶ˆê°€)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  RMSE 0.035 â”€â”€â”€â”€ â”‚  S1~S3 (v25 ë¯¸í¬í•¨)           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- `v25_voltage`ì˜ Capacity ìƒê´€(|r|=0.600) vs ë‹¤ë¥¸ í”¼ì²˜ ìµœëŒ€(|r|=0.177) â†’ **ì •ë³´ëŸ‰ ê²©ì°¨ê°€ ì••ë„ì **
- Phase 8ê¹Œì§€ì˜ KD, Î± íŠœë‹, Feature Augmentation, Optuna Student, Semi-supervised ì „ëµ â†’ **1~4% ë¯¸ë¯¸í•œ ê°œì„ **
- Phase 10ì˜ LambdaKDë¡œ **ìµœëŒ€ +5.64%** ë‹¬ì„±í–ˆìœ¼ë‚˜, ê·¼ë³¸ì  ê²©ì°¨(41.5%)ëŠ” ìœ ì§€
- ì´ëŠ” **ë°ì´í„°ì˜ ë¬¼ë¦¬ì  í•œê³„**ì´ë©° RMSE â‰ˆ 0.034Xê°€ v25 ì—†ì´ ë„ë‹¬ ê°€ëŠ¥í•œ í•˜í•œ

---

## í•µì‹¬ ê²°ë¡  2: S1 Stackingì´ ë¹„ìš© ëŒ€ë¹„ ìµœì„ 

S1(ì…ê³  ì¦‰ì‹œ, 3í”¼ì²˜â†’15í”¼ì²˜ í™•ì¥) + 7ì¢… Stacking ì•™ìƒë¸”ë¡œ **RMSE=0.0346** ë‹¬ì„±:
- S3 Base(0.0348) **ì¶”ì›”** â€” ì¶”ê°€ ì¸¡ì • ì—†ì´ 3.6V ë°©ì „ ê²°ê³¼ë³´ë‹¤ ìš°ìˆ˜
- S2 Base(0.0363) ëŒ€ë¹„ **4.7% ê°œì„ **
- í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ + ì•™ìƒë¸” ì „ëµì˜ íš¨ê³¼ê°€ ì¶”ê°€ ì¸¡ì •ë³´ë‹¤ í° ì‚¬ë¡€

---

## í•µì‹¬ ê²°ë¡  3: KD ì •ë³´ì±„ë„ ë³´ì™„ ì„±ê³µ (Phase 10)

ê¸°ì¡´ Output KDëŠ” Teacher-Student í”¼ì²˜ ê°„ ì •ë³´ ì±„ë„ ë¶€ì¬ë¡œ íš¨ê³¼ê°€ ë¯¸ë¯¸í–ˆìŠµë‹ˆë‹¤.  
Phase 10ì—ì„œ 3ê°€ì§€ ëŒ€ì•ˆ ì±„ë„ì„ ì„¤ê³„í•˜ì—¬ í•œê³„ë¥¼ ë³´ì™„:

| ë°©ë²• | ì „ì´ ëŒ€ìƒ | Best RMSE | ê°œì„ ìœ¨ | í•µì‹¬ ì„±ê³¼ |
|------|----------|-----------|--------|----------|
| **LambdaKD** | ìˆœìœ„ êµ¬ì¡° | S1: 0.0353 | **+5.64%** | Spearman Ï +24% |
| **LambdaKD** | ìˆœìœ„ êµ¬ì¡° | S3: 0.0347 | **+2.49%** | ì „ì²´ KD ìµœê³  RMSE |
| **Bridge** | ê²½ë¡œ ì •ë³´ | S2: 0.0355 | +2.15% | ë‹¨ê³„ì  v25 ì˜ˆì¸¡ |
| **RKD** | ìƒ˜í”Œ ê´€ê³„ | S1: 0.0368 | +1.68% | Anchor ê¸°ë°˜ ê±°ë¦¬ |

> LambdaKD = ìˆœìœ„ ê¸°ë°˜ ì „ì´ë¡œ **ì •ë³´ ì±„ë„ ë¶€ì¬ë¥¼ ìš°íšŒ**í•˜ì—¬ ì‹¤ì§ˆì  ê°œì„  ë‹¬ì„±

---

## ì‹¤ë¬´ ì œì–¸: ì¸¡ì • ì „ëµë³„ ê²½ì œì„± ë¶„ì„

| ì „ëµ | ì¸¡ì • ë²”ìœ„ | ëª¨ë¸ | RMSE (Ah) | ì˜¤ì°¨ìœ¨ | ì†Œìš” ì‹œê°„ | ì¶”ì²œ ëŒ€ìƒ |
|------|----------|------|-----------|--------|----------|----------|
| **A. ì •ë°€ ì˜ˆì¸¡** | S4 (2.5V ì™„ì „ ë°©ì „) | Optuna XGBoost | **0.0245** | 0.49% | ~2ì‹œê°„+ | ê³ ê°€ ì…€, í’ˆì§ˆ ì¸ì¦ |
| **B. ìµœì†Œ ë¹„ìš© ìµœì„ ** | S1 (ì…ê³  ì¦‰ì‹œ) | Stacking ì•™ìƒë¸” | **0.0346** | 0.69% | ~1ë¶„ | â˜… ëŒ€ëŸ‰ ì²˜ë¦¬, ë¹„ìš© ìµœì†Œ |
| **C. S1 + ë“±ê¸‰ íŒë³„** | S1 (ì…ê³  ì¦‰ì‹œ) | LambdaKD | 0.0353 | 0.71% | ~1ë¶„ | ë“±ê¸‰ ë¶„ë¥˜, ìˆœìœ„ ë³´ì¡´ |
| **D. S3 + ìˆœìœ„ KD** | S3 (3.6V ë¶€ë¶„ ë°©ì „) | LambdaKD | **0.0347** | 0.69% | ~1ì‹œê°„ | ì „ì²´ KD ì¤‘ ìµœê³  |
| **E. S2 + ê²½ë¡œ ë³´ì™„** | S2 (4.2V ì¶©ì „) | Bridge-LGB | 0.0355 | 0.71% | ~30ë¶„ | ê²½ë¡œ ì˜ˆì¸¡ í™œìš© |
| **F. ê· í˜• ì˜ˆì¸¡** | S3 (3.6V ë¶€ë¶„ ë°©ì „) | Base LightGBM | 0.0348 | 0.70% | ~1ì‹œê°„ | ì¼ë°˜ ìŠ¤í¬ë¦¬ë‹ |

---

## êµ¬ì²´ì  ê¶Œê³ ì‚¬í•­

### (1) ë‹¨ê¸° ì ìš© (ì¦‰ì‹œ ê°€ëŠ¥)

- **S1 Stacking ëª¨ë¸ë¡œ ì…ê³  ì¦‰ì‹œ ìš©ëŸ‰ ì˜ˆì¸¡**: ì¶”ê°€ ì¸¡ì • ì—†ì´ RMSE 0.0346, ~1ë¶„ ì†Œìš”
- **S4 ê¸°ë°˜ Optuna XGBoostë¡œ ì •ë°€ ê²€ì¦**: RMSE 0.0245 (ê³µì¹­ 5.0Ah ëŒ€ë¹„ 0.49% ì˜¤ì°¨)
- **LambdaKDë¡œ ë“±ê¸‰ íŒë³„**: ì ˆëŒ€ ì •í™•ë„ë³´ë‹¤ ìˆœìœ„ ë³´ì¡´ì´ ì¤‘ìš”í•œ ë¶„ë¥˜ ì—…ë¬´ì— ì í•© (Spearman +24%)

### (2) ì¤‘ê¸° ì ìš© (ê³µì • ìµœì í™”)

- **ì¸¡ì • í”„ë¡œí† ì½œ ì´ì›í™”:**
  - ì „ìˆ˜: S1(ì…ê³  ì¦‰ì‹œ) Stackingìœ¼ë¡œ ë¹ ë¥¸ 1ì°¨ ìŠ¤í¬ë¦¬ë‹ (~1ë¶„/ì…€)
  - í‘œë³¸: S4(2.5V ë°©ì „)ìœ¼ë¡œ ì •ë°€ 2ì°¨ ê²€ì¦ (ì „ì²´ì˜ 10~30%)
  - 1ì°¨ì—ì„œ 'ì˜ì‹¬ ì…€' (ì˜ˆì¸¡ ìš©ëŸ‰ì´ ì„ê³„ê°’ ì´í•˜)ë§Œ 2ì°¨ ì •ë°€ ì¸¡ì •

### (3) ì¥ê¸° ê°œì„  ë°©í–¥

- **ë°©ì „ í”„ë¡œíŒŒì¼ í™•ì¥**: 2.5V ë‹¨ì¼ ì ì´ ì•„ë‹Œ **ë°©ì „ ê³¡ì„  ì „ì²´**(V-t í”„ë¡œíŒŒì¼)ë¥¼ í”¼ì²˜ë¡œ í™œìš©
- **EIS(ì „ê¸°í™”í•™ ì„í”¼ë˜ìŠ¤ ë¶„ê´‘ë²•)**: ë‹¨ì¼ ì£¼íŒŒìˆ˜ AC ì„í”¼ë˜ìŠ¤ ëŒ€ì‹  **ë‹¤ì£¼íŒŒìˆ˜ EIS ìŠ¤í™íŠ¸ëŸ¼** í™œìš©
- **ì˜¨ë„ ë³´ì •**: ì¸¡ì • ì‹œ ì˜¨ë„ ë°ì´í„° ê¸°ë¡ìœ¼ë¡œ ì„í”¼ë˜ìŠ¤/ì „ì•• ë³€ë™ ë³´ì •
- **ë°°ì¹˜ ê°„ ì „ì´í•™ìŠµ**: ë‹¤ë¥¸ ë°°ì¹˜ ë°ì´í„° ì¶•ì  í›„ Domain Adaptation ê¸°ë²•ìœ¼ë¡œ ëª¨ë¸ ë²”ìš©ì„± í™•ë³´

---

## ìµœì¢… ìš”ì•½ ë‹¤ì´ì–´ê·¸ë¨

```
[ì¸¡ì • ë¹„ìš© ì¦ê°€ â†’]

S1(Initial)    S2(+4.2V)    S3(+3.6V)        S4(+2.5V)
  3 í”¼ì²˜          9 í”¼ì²˜         12 í”¼ì²˜             17 í”¼ì²˜
  RÂ²=0.055       RÂ²=0.013      RÂ²=0.092           RÂ²=0.550
  RMSE=0.036     RMSE=0.036    RMSE=0.035         RMSE=0.025

   â† â”€ â”€ â”€ â”€ ê±°ì˜ ë™ì¼ â”€ â”€ â”€ â”€ â†’     â”‚â† ê¸‰ê²©í•œ ì„±ëŠ¥ í–¥ìƒ â†’â”‚

  â˜… Phase 9: S1 Stacking RMSE=0.0346 â†’ S3 Base ì¶”ì›”!
  â˜… Phase 10: LambdaKD  S1=0.0353(+5.64%), S3=0.0347(+2.49%)
  â˜… í•µì‹¬ í”¼ì²˜: v25_voltage (|r|=0.600)
  â˜… ì”ì—¬ ê²©ì°¨: 41.5% (ì •ë³´ ì´ë¡ ì  í•œê³„)
```

---

## ìƒì„±ëœ ì‹œê°í™” íŒŒì¼ ëª©ë¡ (21ê°œ)

| íŒŒì¼ëª… | Phase | ë‚´ìš© |
|--------|-------|------|
| `eda_01_distributions.png` | 1 | í”¼ì²˜ë³„ íˆìŠ¤í† ê·¸ë¨ + KDE ë¶„í¬ |
| `eda_02_boxplots.png` | 1 | í”¼ì²˜ë³„ ë°•ìŠ¤í”Œë¡¯ (IQR ì´ìƒì¹˜) |
| `eda_03_correlation.png` | 1 | Pearson/Spearman ìƒê´€ íˆíŠ¸ë§µ |
| `eda_04_scatter.png` | 1 | í”¼ì²˜ vs Capacity ì‚°ì ë„ (ì¶”ì„¸ì„ ) |
| `eda_05_outliers.png` | 1 | ì´ìƒì¹˜ í•˜ì´ë¼ì´íŠ¸ ì‚°ì ë„ |
| `eda_06_feature_groups.png` | 1 | ì¸¡ì • êµ¬ê°„ë³„ ì „ì••/ì„í”¼ë˜ìŠ¤ ë°•ìŠ¤í”Œë¡¯ |
| `eda_07_pairplot.png` | 1 | ì„í”¼ë˜ìŠ¤ í”¼ì²˜ Pairplot |
| `phase2_derived_vs_capacity.png` | 2 | 16ê°œ íŒŒìƒë³€ìˆ˜ vs Capacity ì‚°ì ë„ |
| `phase2_correlation_heatmap.png` | 2 | íŒŒìƒë³€ìˆ˜ ê°„ ìƒê´€ íˆíŠ¸ë§µ |
| `phase3_cleaned_correlation.png` | 3 | ì •ì œ í›„ í”¼ì²˜ ìƒê´€ íˆíŠ¸ë§µ + ë°” ì°¨íŠ¸ |
| `phase3_derived_scatter.png` | 3 | ìœ íš¨ íŒŒìƒë³€ìˆ˜ vs Capacity ì‚°ì ë„ |
| `phase3_vif_comparison.png` | 3 | VIF ë¹„êµ (ì œê±° ì „/í›„) |
| `phase4_data_split.png` | 4 | Train/Test ë¶„í¬ ì‹œê°í™”(4ê°œ ì°¨íŠ¸) |
| `phase5_rmse_barplot.png` | 5 | ì‹œë‚˜ë¦¬ì˜¤Ã—ëª¨ë¸ RMSE ë§‰ëŒ€ê·¸ë˜í”„ |
| `phase5_r2_heatmap.png` | 5 | ì‹œë‚˜ë¦¬ì˜¤Ã—ëª¨ë¸ RÂ² íˆíŠ¸ë§µ |
| `phase5_feature_importance.png` | 5 | í”¼ì²˜ ì¤‘ìš”ë„ ì°¨íŠ¸ |
| `phase8_alpha_search.png` | 8-2 | Î± ê·¸ë¦¬ë“œ íƒìƒ‰ ê²°ê³¼ (S2/S3) |
| `phase8_semi_supervised.png` | 8-4 | Semi-supervised ì„±ëŠ¥ + ë¹„ìš©íš¨ìœ¨ |
| `phase8_comparison.png` | 8-5 | Phase 7-8 ì „ì²´ ì¢…í•© ë¹„êµ |
| `phase9_s1_comparison.png` | 9 | S1 ì§‘ì¤‘ ê°œì„  ì¢…í•© ë¹„êµ |
| `phase10_kd_channel.png` | 10 | KD ì •ë³´ì±„ë„ ë³´ì™„ 4-íŒ¨ë„ ì¢…í•© |

---

## ë¶€ë¡: ë¶„ì„ í™˜ê²½ ìƒì„¸

| í•­ëª© | ë²„ì „/ì‚¬ì–‘ |
|------|----------|
| Python | 3.14.2 |
| XGBoost | 3.2.0 |
| LightGBM | 4.6.0 |
| Optuna | 4.7.0 |
| scikit-learn | 1.8.0 |
| pandas | 2.3.0 |
| numpy | 2.3.0 |
| matplotlib | 3.10.1 |
| seaborn | 0.13.2 |

### Optuna Teacher ìµœì  íŒŒë¼ë¯¸í„°

```python
{
    'n_estimators': 653,
    'max_depth': 7,
    'learning_rate': 0.182,
    'subsample': 0.835,
    'colsample_bytree': 0.810,
    'reg_alpha': 0.002,
    'reg_lambda': 3.887,
    'objective': 'reg:squarederror'
}
# Validation RMSE: 0.0195
# Test RMSE: 0.0245
```

### Knowledge Distillation ìˆ˜ì‹

$$y_{distilled} = \alpha \cdot y_{true} + (1 - \alpha) \cdot y_{teacher\_pred}$$

- $\alpha = 1.0$: ì¼ë°˜ í•™ìŠµ (KD ì—†ìŒ)
- $\alpha = 0.0$: Teacher ì˜ˆì¸¡ë§Œ í•™ìŠµ
- $\alpha = 0.5$: Hard/Soft Label ë™ì¼ ë¹„ì¤‘ (ê¸°ë³¸ ì„¤ì •)

### Phase 10 LambdaKD ìˆ˜ì‹

$$L_{total} = L_{RMSE} + \lambda \cdot L_{Spearman}$$

- $L_{RMSE}$: ì¼ë°˜ íšŒê·€ ì†ì‹¤ (Student ì˜ˆì¸¡ vs Blended Target)
- $L_{Spearman}$: Teacher â†” Student ìˆœìœ„ ìƒê´€ ì†ì‹¤ ($1 - \rho_{Spearman}$)
- $\lambda$: Optuna ìµœì í™” (ìµœì  0.41~0.70)
- S1 ìµœì : Î»=0.41, Î±=0.693, Spearman Ï=0.32
- S3 ìµœì : Î»=0.70, Î±=0.45, Spearman Ï=0.40

### Phase 9 Stacking ì•™ìƒë¸” êµ¬ì„±

```python
# 7ì¢… Base Learner (í™•ì¥ S1, 15í”¼ì²˜)
base_models = [
    ('lgb', LGBMRegressor(n_estimators=478, max_depth=6, lr=0.048)),
    ('xgb', XGBRegressor(n_estimators=412, max_depth=5, lr=0.063)),
    ('gb', GradientBoostingRegressor(n_estimators=367, max_depth=4)),
    ('rf', RandomForestRegressor(n_estimators=300)),
    ('svr', SVR(C=1.0, epsilon=0.01)),
    ('ridge', Ridge(alpha=1.0)),
    ('knn', KNeighborsRegressor(n_neighbors=20, weights='distance'))
]
# Meta-learner: RidgeCV (5-fold OOF predictions â†’ ì•™ìƒë¸”)
# S1 Stacking RMSE = 0.0346, RÂ² = 0.1028
```

---

> **ë³¸ ë³´ê³ ì„œì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.**
